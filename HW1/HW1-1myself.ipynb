{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = 10\n",
    "\n",
    "test = np.load('test.npz')\n",
    "train = np.load('train.npz')\n",
    "\n",
    "y_test = test['label']\n",
    "x_test = test['image']\n",
    "y_train = train['label']\n",
    "x_train = train['image']\n",
    "\n",
    "x_train = x_train.reshape(-1, x_train.shape[1] * x_train.shape[2])\n",
    "x_test = x_test.reshape(-1, x_test.shape[1] * x_test.shape[2])\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "X = np.vstack((x_train, x_test))\n",
    "y = np.vstack((y_train, y_test))\n",
    "\n",
    "examples = y.shape[0]\n",
    "y = y.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "  #print(y)\n",
    "\n",
    "m = x_train.shape[0]\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:, :m], Y_new[:, m:]\n",
    "\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "#     # To prevent from overflow\n",
    "#     z = np.clip(z, 1e-15, 1 - 1e-15)\n",
    "    s = sigmoid(z) * (1 - sigmoid(z))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y, Y_hat):\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(Y, Y_hat):\n",
    "    L = Y_hat - Y\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    s = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predicts, golds):\n",
    "    correct = 0\n",
    "    total = len(predicts)\n",
    "    assert len(predicts) == len(golds)\n",
    "    for predict, gold in zip(predicts, golds):\n",
    "        if predict == gold:\n",
    "            correct += 1\n",
    "    accurancy = correct / total\n",
    "    return accurancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_, output):\n",
    "        self.input = input_\n",
    "        self.output = output  # number of layer node\n",
    "        self.W = np.random.randn(self.output, self.input) * np.sqrt(1. / self.input)\n",
    "        self.b = np.zeros((self.output, 1)) * np.sqrt(1. / self.input)\n",
    "        \n",
    "    def forward(self, last_layer):\n",
    "        self.last_layer = last_layer\n",
    "        layer_output = np.matmul(self.W, self.last_layer) + self.b\n",
    "#         layer_output = sigmoid(layer_output_temp)\n",
    "        return layer_output\n",
    "    \n",
    "    def back_propagation(self, CE_gradientorgradient, m_batch, learning_rate):\n",
    "        W_temp = self.W\n",
    "        W_gradient = (1. / m_batch) * np.matmul(CE_gradientorgradient, self.last_layer.T)\n",
    "        b_gradient = (1. / m_batch) * np.sum(CE_gradientorgradient, axis=1, keepdims=True)\n",
    "        self.W_new = self.W - learning_rate * W_gradient\n",
    "        self.b_new = self.b - learning_rate * b_gradient\n",
    "        self.W = self.W_new\n",
    "        self.b = self.b_new\n",
    "        gradient_temp = np.matmul(W_temp.T, CE_gradientorgradient)\n",
    "        return gradient_temp\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenlayer1 = Layer(784, 400)\n",
    "hiddenlayer2 = Layer(400, 400)\n",
    "outputlayer = Layer(400, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_train_epoch(X_train, Y_train, batch_size = 64, epoch = 10, learning_rate = 0.03):\n",
    "    TrainError = []\n",
    "    TestError = []\n",
    "    for i in range(epoch):\n",
    "       \n",
    "        # shuffle training set\n",
    "        permutation = np.random.permutation(X_train.shape[1])\n",
    "        X_train_shuffled = X_train[:, permutation]\n",
    "        Y_train_shuffled = Y_train[:, permutation]\n",
    "    \n",
    "        batch_num = len(X_train) // batch_size\n",
    "        predicts = []\n",
    "        golds = []\n",
    "        predicts_test = []\n",
    "        golds_test = []\n",
    "        \n",
    "        for j in range(batch_num):\n",
    "            begin = j * batch_size\n",
    "            end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "            X = X_train[:, begin:end]\n",
    "            Y = Y_train[:, begin:end]\n",
    "            m_batch = end - begin\n",
    "            \n",
    "            output1_temp = hiddenlayer1.forward(X)\n",
    "            output1 = sigmoid(output1_temp)\n",
    "            output2_temp = hiddenlayer2.forward(output1)\n",
    "            output2 = sigmoid(output2_temp)\n",
    "            y_hat_temp = outputlayer.forward(output2)\n",
    "            y_hat = softmax(y_hat_temp)\n",
    "            #print(y_hat)\n",
    "            \n",
    "            predicts += np.argmax(y_hat, axis = 0).tolist()\n",
    "            golds += np.argmax(Y, axis = 0).tolist()\n",
    "            \n",
    "            \n",
    "            loss = cross_entropy(Y, y_hat)\n",
    "            gradient = cross_entropy_gradient(Y, y_hat)\n",
    "            \n",
    "            back_output1 = outputlayer.back_propagation(gradient, m_batch, learning_rate)\n",
    "            back_output2_temp = sigmoid_gradient(output2_temp) * back_output1\n",
    "            back_output2 = hiddenlayer2.back_propagation(back_output2_temp, m_batch, learning_rate)\n",
    "            back_output3_temp = sigmoid_gradient(output1_temp) * back_output2\n",
    "            back_output3 = hiddenlayer1.back_propagation(back_output3_temp, m_batch, learning_rate)\n",
    "            \n",
    "            #---------test data-----------\n",
    "            \n",
    "            \n",
    "            output1_temp = hiddenlayer1.forward(X_test)\n",
    "            output1 = sigmoid(output1_temp)\n",
    "            output2_temp = hiddenlayer2.forward(output1)\n",
    "            output2 = sigmoid(output2_temp)\n",
    "            y_hat_temp = outputlayer.forward(output2)\n",
    "            y_hat = softmax(y_hat_temp)\n",
    "#             print(y_hat.shape)\n",
    "            #print(y_hat)\n",
    "#             print(Y_test.shape)\n",
    "            predicts_test += np.argmax(y_hat, axis=0).tolist()\n",
    "            golds_test += np.argmax(Y_test, axis=0).tolist()\n",
    "            \n",
    "            \n",
    "#             loss_test = cross_entropy(Y_test, y_hat)\n",
    "            \n",
    "        print('Epoch : ', i + 1, 'training_loss = ', loss, 'train_accur = ', evaluation(predicts, golds), 'test_accur = ', evaluation(predicts_test, golds_test))\n",
    "            \n",
    "        TrainError.append(1 - evaluation(predicts, golds))\n",
    "        TestError.append(1 - evaluation(predicts_test, golds_test))    \n",
    "\n",
    "    with open(\"Train_error_rate.json\", mode=\"w\") as stream:\n",
    "        json.dump(TrainError, stream)\n",
    "\n",
    "    with open(\"Test_error_rate.json\", mode=\"w\") as stream:\n",
    "        json.dump(TestError, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 training_loss =  2.2731195478764694 train_accur =  0.13411458333333334 test_accur =  0.13793053629218677\n",
      "Epoch :  2 training_loss =  2.181276184156391 train_accur =  0.2903645833333333 test_accur =  0.2922590152565881\n",
      "Epoch :  3 training_loss =  2.1029575278879085 train_accur =  0.4427083333333333 test_accur =  0.40812817845584837\n",
      "Epoch :  4 training_loss =  2.016173813268807 train_accur =  0.53125 test_accur =  0.49195272769301895\n",
      "Epoch :  5 training_loss =  1.9342083408216966 train_accur =  0.60546875 test_accur =  0.5476623901987979\n",
      "Epoch :  6 training_loss =  1.854205862288385 train_accur =  0.6393229166666666 test_accur =  0.5906004392048081\n",
      "Epoch :  7 training_loss =  1.7769694915441547 train_accur =  0.6744791666666666 test_accur =  0.622298312528895\n",
      "Epoch :  8 training_loss =  1.6987012433166848 train_accur =  0.7057291666666666 test_accur =  0.6495607951918632\n",
      "Epoch :  9 training_loss =  1.619306601367331 train_accur =  0.73828125 test_accur =  0.6697439898289412\n",
      "Epoch :  10 training_loss =  1.5373112755209133 train_accur =  0.7578125 test_accur =  0.6869654415164124\n",
      "Epoch :  11 training_loss =  1.460315083710059 train_accur =  0.76953125 test_accur =  0.7043602635228848\n",
      "Epoch :  12 training_loss =  1.38833313151398 train_accur =  0.7916666666666666 test_accur =  0.7167562413314841\n",
      "Epoch :  13 training_loss =  1.3126792377777554 train_accur =  0.8125 test_accur =  0.7293689320388349\n",
      "Epoch :  14 training_loss =  1.2401975503096758 train_accur =  0.8268229166666666 test_accur =  0.7385575589459085\n",
      "Epoch :  15 training_loss =  1.177005340207245 train_accur =  0.8424479166666666 test_accur =  0.7485985899214055\n",
      "Epoch :  16 training_loss =  1.1113442025153546 train_accur =  0.8515625 test_accur =  0.7569492602866389\n",
      "Epoch :  17 training_loss =  1.0459465288715937 train_accur =  0.8567708333333334 test_accur =  0.7619914470642626\n",
      "Epoch :  18 training_loss =  0.9856934524216241 train_accur =  0.8697916666666666 test_accur =  0.7696919787332409\n",
      "Epoch :  19 training_loss =  0.927583390835255 train_accur =  0.8776041666666666 test_accur =  0.7761500231160425\n",
      "Epoch :  20 training_loss =  0.8730120234837204 train_accur =  0.8919270833333334 test_accur =  0.7807732316227461\n",
      "Epoch :  21 training_loss =  0.830401174211244 train_accur =  0.89453125 test_accur =  0.7852953074433657\n",
      "Epoch :  22 training_loss =  0.782394484936024 train_accur =  0.8971354166666666 test_accur =  0.7890083217753121\n",
      "Epoch :  23 training_loss =  0.7459225366508855 train_accur =  0.9036458333333334 test_accur =  0.7918978270920018\n",
      "Epoch :  24 training_loss =  0.7086692971951594 train_accur =  0.9088541666666666 test_accur =  0.7943683541377716\n",
      "Epoch :  25 training_loss =  0.669359725420891 train_accur =  0.9166666666666666 test_accur =  0.7964632454923717\n",
      "Epoch :  26 training_loss =  0.6389847949400299 train_accur =  0.9192708333333334 test_accur =  0.8002340499306518\n",
      "Epoch :  27 training_loss =  0.6080179148904212 train_accur =  0.9205729166666666 test_accur =  0.8020110957004161\n",
      "Epoch :  28 training_loss =  0.5744263143793831 train_accur =  0.92578125 test_accur =  0.8052184466019418\n",
      "Epoch :  29 training_loss =  0.5480888274873219 train_accur =  0.9309895833333334 test_accur =  0.8063886962552012\n",
      "Epoch :  30 training_loss =  0.5233038777534536 train_accur =  0.9296875 test_accur =  0.8083246648173833\n",
      "Epoch :  31 training_loss =  0.504424085440041 train_accur =  0.9348958333333334 test_accur =  0.8089892510402219\n",
      "Epoch :  32 training_loss =  0.4787469521485439 train_accur =  0.9348958333333334 test_accur =  0.8109685621821544\n",
      "Epoch :  33 training_loss =  0.4573445446294889 train_accur =  0.9388020833333334 test_accur =  0.8131790337494221\n",
      "Epoch :  34 training_loss =  0.440388896444339 train_accur =  0.9440104166666666 test_accur =  0.8141036754507628\n",
      "Epoch :  35 training_loss =  0.4229324978923975 train_accur =  0.94140625 test_accur =  0.8154906380027739\n",
      "Epoch :  36 training_loss =  0.407018934809062 train_accur =  0.9466145833333334 test_accur =  0.8164297272306981\n",
      "Epoch :  37 training_loss =  0.39041854556914846 train_accur =  0.9518229166666666 test_accur =  0.8173977115117892\n",
      "Epoch :  38 training_loss =  0.37613807829098966 train_accur =  0.9518229166666666 test_accur =  0.8170654184003698\n",
      "Epoch :  39 training_loss =  0.3616319993577374 train_accur =  0.953125 test_accur =  0.8187557790106333\n",
      "Epoch :  40 training_loss =  0.35173812356261025 train_accur =  0.9557291666666666 test_accur =  0.8188858067498844\n",
      "Epoch :  41 training_loss =  0.3394673482296828 train_accur =  0.95703125 test_accur =  0.8198104484512252\n",
      "Epoch :  42 training_loss =  0.32633456003710093 train_accur =  0.9583333333333334 test_accur =  0.8197093157651411\n",
      "Epoch :  43 training_loss =  0.3159069498986218 train_accur =  0.9609375 test_accur =  0.82083622283865\n",
      "Epoch :  44 training_loss =  0.30597996985943676 train_accur =  0.9622395833333334 test_accur =  0.8220209200184928\n",
      "Epoch :  45 training_loss =  0.2974253958444576 train_accur =  0.9622395833333334 test_accur =  0.8226132686084142\n",
      "Epoch :  46 training_loss =  0.28724626701650596 train_accur =  0.9622395833333334 test_accur =  0.823826860841424\n",
      "Epoch :  47 training_loss =  0.278895761403959 train_accur =  0.9609375 test_accur =  0.8242024965325936\n",
      "Epoch :  48 training_loss =  0.2711278768928884 train_accur =  0.9622395833333334 test_accur =  0.8239857836338419\n",
      "Epoch :  49 training_loss =  0.26100757765843086 train_accur =  0.9622395833333334 test_accur =  0.8246359223300971\n",
      "Epoch :  50 training_loss =  0.25242246505253507 train_accur =  0.9622395833333334 test_accur =  0.8250115580212668\n",
      "Epoch :  51 training_loss =  0.2446475538756932 train_accur =  0.9635416666666666 test_accur =  0.8249393203883495\n",
      "Epoch :  52 training_loss =  0.23808432974546018 train_accur =  0.9635416666666666 test_accur =  0.8247803975959316\n",
      "Epoch :  53 training_loss =  0.2325193735117096 train_accur =  0.9661458333333334 test_accur =  0.8251126907073509\n",
      "Epoch :  54 training_loss =  0.22635087437489249 train_accur =  0.9674479166666666 test_accur =  0.8253294036061026\n",
      "Epoch :  55 training_loss =  0.220537604846542 train_accur =  0.9661458333333334 test_accur =  0.8260373324086916\n",
      "Epoch :  56 training_loss =  0.21595774977958967 train_accur =  0.96875 test_accur =  0.826080674988442\n",
      "Epoch :  57 training_loss =  0.2109958495131095 train_accur =  0.96875 test_accur =  0.826499653259362\n",
      "Epoch :  58 training_loss =  0.20587515862386496 train_accur =  0.97265625 test_accur =  0.8263551779935275\n",
      "Epoch :  59 training_loss =  0.20105173039693663 train_accur =  0.9713541666666666 test_accur =  0.8270053166897827\n",
      "Epoch :  60 training_loss =  0.1960173118189094 train_accur =  0.97265625 test_accur =  0.826629680998613\n",
      "Epoch :  61 training_loss =  0.19068962889439683 train_accur =  0.97265625 test_accur =  0.8271931345353676\n",
      "Epoch :  62 training_loss =  0.1867877260728364 train_accur =  0.9739583333333334 test_accur =  0.8277421405455386\n",
      "Epoch :  63 training_loss =  0.1821617235467852 train_accur =  0.9739583333333334 test_accur =  0.8280166435506241\n",
      "Epoch :  64 training_loss =  0.1773862586789003 train_accur =  0.97265625 test_accur =  0.828493411927878\n",
      "Epoch :  65 training_loss =  0.17208587318893162 train_accur =  0.97265625 test_accur =  0.8286956773000462\n",
      "Epoch :  66 training_loss =  0.1688341951961332 train_accur =  0.9739583333333334 test_accur =  0.8290568654646324\n",
      "Epoch :  67 training_loss =  0.16544904749509554 train_accur =  0.97265625 test_accur =  0.8283778317152104\n",
      "Epoch :  68 training_loss =  0.1622229217035518 train_accur =  0.9739583333333334 test_accur =  0.8287968099861304\n",
      "Epoch :  69 training_loss =  0.15796616248242357 train_accur =  0.9752604166666666 test_accur =  0.8287245723532132\n",
      "Epoch :  70 training_loss =  0.15577165462596754 train_accur =  0.9765625 test_accur =  0.8291724456773001\n",
      "Epoch :  71 training_loss =  0.1522589730721733 train_accur =  0.9765625 test_accur =  0.8296058714748035\n",
      "Epoch :  72 training_loss =  0.14877592445244436 train_accur =  0.9765625 test_accur =  0.8296636615811374\n",
      "Epoch :  73 training_loss =  0.1459469247579158 train_accur =  0.9765625 test_accur =  0.8295336338418863\n",
      "Epoch :  74 training_loss =  0.1431645329513846 train_accur =  0.9765625 test_accur =  0.8297358992140546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  75 training_loss =  0.14052604653412532 train_accur =  0.9778645833333334 test_accur =  0.8300248497457235\n",
      "Epoch :  76 training_loss =  0.13806456063736688 train_accur =  0.9778645833333334 test_accur =  0.8307761211280629\n",
      "Epoch :  77 training_loss =  0.13551097462930614 train_accur =  0.9778645833333334 test_accur =  0.8309494914470643\n",
      "Epoch :  78 training_loss =  0.13315077611192444 train_accur =  0.9791666666666666 test_accur =  0.8310795191863153\n",
      "Epoch :  79 training_loss =  0.13126328770088413 train_accur =  0.9791666666666666 test_accur =  0.8309350439204808\n",
      "Epoch :  80 training_loss =  0.12860784638982226 train_accur =  0.98046875 test_accur =  0.831166204345816\n",
      "Epoch :  81 training_loss =  0.12628020693888653 train_accur =  0.9791666666666666 test_accur =  0.8313684697179843\n",
      "Epoch :  82 training_loss =  0.12382991301142762 train_accur =  0.9791666666666666 test_accur =  0.831585182616736\n",
      "Epoch :  83 training_loss =  0.12152788781731998 train_accur =  0.98046875 test_accur =  0.8320330559408229\n",
      "Epoch :  84 training_loss =  0.11976705190073857 train_accur =  0.9830729166666666 test_accur =  0.8323942441054092\n",
      "Epoch :  85 training_loss =  0.11803754688097869 train_accur =  0.9830729166666666 test_accur =  0.832293111419325\n",
      "Epoch :  86 training_loss =  0.11624400051427855 train_accur =  0.9830729166666666 test_accur =  0.8322642163661581\n",
      "Epoch :  87 training_loss =  0.11460213753301918 train_accur =  0.9830729166666666 test_accur =  0.8320330559408229\n",
      "Epoch :  88 training_loss =  0.11285155521520548 train_accur =  0.9830729166666666 test_accur =  0.8322642163661581\n",
      "Epoch :  89 training_loss =  0.11080066077930699 train_accur =  0.9830729166666666 test_accur =  0.8320908460471568\n",
      "Epoch :  90 training_loss =  0.10895886249041986 train_accur =  0.9830729166666666 test_accur =  0.8328999075358299\n",
      "Epoch :  91 training_loss =  0.10714530462613239 train_accur =  0.9830729166666666 test_accur =  0.8331455154877485\n",
      "Epoch :  92 training_loss =  0.10614837054218051 train_accur =  0.9830729166666666 test_accur =  0.8334778085991679\n",
      "Epoch :  93 training_loss =  0.10304364778356559 train_accur =  0.9830729166666666 test_accur =  0.8335355987055016\n",
      "Epoch :  94 training_loss =  0.10194193636524732 train_accur =  0.984375 test_accur =  0.8336511789181692\n",
      "Epoch :  95 training_loss =  0.10010641048995048 train_accur =  0.984375 test_accur =  0.8339545769764216\n",
      "Epoch :  96 training_loss =  0.09841118726799414 train_accur =  0.984375 test_accur =  0.8338101017105871\n",
      "Epoch :  97 training_loss =  0.096939702474533 train_accur =  0.984375 test_accur =  0.8342146324549237\n",
      "Epoch :  98 training_loss =  0.09556933299558609 train_accur =  0.984375 test_accur =  0.8344024503005085\n",
      "Epoch :  99 training_loss =  0.09413690897576141 train_accur =  0.984375 test_accur =  0.8345035829865927\n",
      "Epoch :  100 training_loss =  0.09278294495135025 train_accur =  0.9856770833333334 test_accur =  0.83457582061951\n",
      "Epoch :  101 training_loss =  0.09141777559911135 train_accur =  0.9869791666666666 test_accur =  0.8344891354600092\n",
      "Epoch :  102 training_loss =  0.09010357984213041 train_accur =  0.98828125 test_accur =  0.8346191631992603\n",
      "Epoch :  103 training_loss =  0.08885961284894262 train_accur =  0.9895833333333334 test_accur =  0.8347780859916782\n",
      "Epoch :  104 training_loss =  0.08752971663282158 train_accur =  0.9895833333333334 test_accur =  0.8348792186777624\n",
      "Epoch :  105 training_loss =  0.086311750132226 train_accur =  0.9895833333333334 test_accur =  0.8347780859916782\n",
      "Epoch :  106 training_loss =  0.08520182761328626 train_accur =  0.9895833333333334 test_accur =  0.8349803513638465\n",
      "Epoch :  107 training_loss =  0.08410734616005275 train_accur =  0.9895833333333334 test_accur =  0.8349370087840962\n",
      "Epoch :  108 training_loss =  0.08303971810050298 train_accur =  0.9908854166666666 test_accur =  0.8350959315765141\n",
      "Epoch :  109 training_loss =  0.08201056385575367 train_accur =  0.9908854166666666 test_accur =  0.8352693018955155\n",
      "Epoch :  110 training_loss =  0.08100689072220754 train_accur =  0.9908854166666666 test_accur =  0.8354282246879334\n",
      "Epoch :  111 training_loss =  0.08000702598585924 train_accur =  0.9908854166666666 test_accur =  0.8354426722145168\n",
      "Epoch :  112 training_loss =  0.07903366567350709 train_accur =  0.9908854166666666 test_accur =  0.8354860147942672\n",
      "Epoch :  113 training_loss =  0.07806990693403437 train_accur =  0.9921875 test_accur =  0.8352693018955155\n",
      "Epoch :  114 training_loss =  0.07719624106555126 train_accur =  0.9921875 test_accur =  0.8356160425335183\n",
      "Epoch :  115 training_loss =  0.07612544330307577 train_accur =  0.9921875 test_accur =  0.8355004623208506\n",
      "Epoch :  116 training_loss =  0.07525341686136147 train_accur =  0.9921875 test_accur =  0.8353415395284327\n",
      "Epoch :  117 training_loss =  0.07425812690403161 train_accur =  0.9934895833333334 test_accur =  0.8356738326398521\n",
      "Epoch :  118 training_loss =  0.07342251005159553 train_accur =  0.9934895833333334 test_accur =  0.8356738326398521\n",
      "Epoch :  119 training_loss =  0.07241441499485893 train_accur =  0.9934895833333334 test_accur =  0.8361794960702728\n",
      "Epoch :  120 training_loss =  0.07155399834570395 train_accur =  0.9934895833333334 test_accur =  0.8362950762829403\n",
      "Epoch :  121 training_loss =  0.07072848051463003 train_accur =  0.9934895833333334 test_accur =  0.8364251040221914\n",
      "Epoch :  122 training_loss =  0.06989270247864725 train_accur =  0.9934895833333334 test_accur =  0.8364684466019418\n",
      "Epoch :  123 training_loss =  0.06906067083502528 train_accur =  0.9934895833333334 test_accur =  0.8365551317614425\n",
      "Epoch :  124 training_loss =  0.06838899644517858 train_accur =  0.9934895833333334 test_accur =  0.836959662505779\n",
      "Epoch :  125 training_loss =  0.06751539817618729 train_accur =  0.9934895833333334 test_accur =  0.8372775080906149\n",
      "Epoch :  126 training_loss =  0.06672843236266959 train_accur =  0.9934895833333334 test_accur =  0.8372341655108645\n",
      "Epoch :  127 training_loss =  0.06603668822239106 train_accur =  0.9934895833333334 test_accur =  0.8372775080906149\n",
      "Epoch :  128 training_loss =  0.06529431309123059 train_accur =  0.9947916666666666 test_accur =  0.8375664586222838\n",
      "Epoch :  129 training_loss =  0.06465513690876723 train_accur =  0.9947916666666666 test_accur =  0.8375809061488673\n",
      "Epoch :  130 training_loss =  0.06388522828722044 train_accur =  0.9947916666666666 test_accur =  0.8376242487286176\n",
      "Epoch :  131 training_loss =  0.06320639576172077 train_accur =  0.9947916666666666 test_accur =  0.8379420943134536\n",
      "Epoch :  132 training_loss =  0.06256215173051724 train_accur =  0.9947916666666666 test_accur =  0.8380287794729542\n",
      "Epoch :  133 training_loss =  0.06190325467055906 train_accur =  0.9947916666666666 test_accur =  0.83792764678687\n",
      "Epoch :  134 training_loss =  0.061213247269398294 train_accur =  0.9947916666666666 test_accur =  0.8378987517337032\n",
      "Epoch :  135 training_loss =  0.0605989529369161 train_accur =  0.9947916666666666 test_accur =  0.8379709893666204\n",
      "Epoch :  136 training_loss =  0.059998909173137285 train_accur =  0.9947916666666666 test_accur =  0.8381588072122053\n",
      "Epoch :  137 training_loss =  0.059409693091117 train_accur =  0.9947916666666666 test_accur =  0.8383032824780398\n",
      "Epoch :  138 training_loss =  0.058915380946930954 train_accur =  0.9947916666666666 test_accur =  0.8383177300046232\n",
      "Epoch :  139 training_loss =  0.05815031802393909 train_accur =  0.9947916666666666 test_accur =  0.838534442903375\n",
      "Epoch :  140 training_loss =  0.05753423091565168 train_accur =  0.9947916666666666 test_accur =  0.8385633379565418\n",
      "Epoch :  141 training_loss =  0.05696234862681418 train_accur =  0.9947916666666666 test_accur =  0.838505547850208\n",
      "Epoch :  142 training_loss =  0.05638688755582076 train_accur =  0.9947916666666666 test_accur =  0.8384622052704577\n",
      "Epoch :  143 training_loss =  0.05573781875163035 train_accur =  0.9947916666666666 test_accur =  0.8384911003236246\n",
      "Epoch :  144 training_loss =  0.055288604034815066 train_accur =  0.99609375 test_accur =  0.8388378409616274\n",
      "Epoch :  145 training_loss =  0.05451362179277906 train_accur =  0.99609375 test_accur =  0.8392712667591309\n",
      "Epoch :  146 training_loss =  0.05379634007643523 train_accur =  0.99609375 test_accur =  0.8392568192325474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  147 training_loss =  0.05325343341498378 train_accur =  0.99609375 test_accur =  0.8391990291262136\n",
      "Epoch :  148 training_loss =  0.05278102283838846 train_accur =  0.99609375 test_accur =  0.8393001618122977\n",
      "Epoch :  149 training_loss =  0.052179819037454944 train_accur =  0.99609375 test_accur =  0.8393868469717984\n",
      "Epoch :  150 training_loss =  0.05165711382397484 train_accur =  0.99609375 test_accur =  0.8394157420249653\n",
      "Epoch :  151 training_loss =  0.05111789547510048 train_accur =  0.99609375 test_accur =  0.8395891123439667\n",
      "Epoch :  152 training_loss =  0.0505426041581807 train_accur =  0.99609375 test_accur =  0.8398347202958854\n",
      "Epoch :  153 training_loss =  0.05001743670923284 train_accur =  0.99609375 test_accur =  0.8399069579288025\n",
      "Epoch :  154 training_loss =  0.04948942569624103 train_accur =  0.99609375 test_accur =  0.8400947757743874\n",
      "Epoch :  155 training_loss =  0.04900691208769219 train_accur =  0.9973958333333334 test_accur =  0.8401814609338881\n",
      "Epoch :  156 training_loss =  0.048560213827777546 train_accur =  0.9973958333333334 test_accur =  0.8402825936199723\n",
      "Epoch :  157 training_loss =  0.04810002328778554 train_accur =  0.9986979166666666 test_accur =  0.8403548312528895\n",
      "Epoch :  158 training_loss =  0.0476089015937267 train_accur =  0.9986979166666666 test_accur =  0.8403259361997226\n",
      "Epoch :  159 training_loss =  0.04707193776827674 train_accur =  0.9986979166666666 test_accur =  0.8402825936199723\n",
      "Epoch :  160 training_loss =  0.04661922810646759 train_accur =  0.9986979166666666 test_accur =  0.8402103559870551\n",
      "Epoch :  161 training_loss =  0.04618649127835327 train_accur =  0.9986979166666666 test_accur =  0.8403981738326398\n",
      "Epoch :  162 training_loss =  0.04576946732925341 train_accur =  0.9986979166666666 test_accur =  0.8405137540453075\n",
      "Epoch :  163 training_loss =  0.04536594480663062 train_accur =  0.9986979166666666 test_accur =  0.8404848589921405\n",
      "Epoch :  164 training_loss =  0.04494713021352412 train_accur =  0.9986979166666666 test_accur =  0.8404848589921405\n",
      "Epoch :  165 training_loss =  0.044556248826790254 train_accur =  0.9986979166666666 test_accur =  0.840499306518724\n",
      "Epoch :  166 training_loss =  0.044168896613191205 train_accur =  0.9986979166666666 test_accur =  0.8404415164123902\n",
      "Epoch :  167 training_loss =  0.043801015161789074 train_accur =  0.9986979166666666 test_accur =  0.8404126213592233\n",
      "Epoch :  168 training_loss =  0.04343415825661475 train_accur =  0.9986979166666666 test_accur =  0.8405282015718909\n",
      "Epoch :  169 training_loss =  0.04308198971535671 train_accur =  0.9986979166666666 test_accur =  0.8405570966250578\n",
      "Epoch :  170 training_loss =  0.042733401519988445 train_accur =  0.9986979166666666 test_accur =  0.8406726768377254\n",
      "Epoch :  171 training_loss =  0.042386961079857385 train_accur =  0.9986979166666666 test_accur =  0.8407015718908922\n",
      "Epoch :  172 training_loss =  0.042047354125756026 train_accur =  0.9986979166666666 test_accur =  0.8407449144706426\n",
      "Epoch :  173 training_loss =  0.0417096891545168 train_accur =  0.9986979166666666 test_accur =  0.840658229311142\n",
      "Epoch :  174 training_loss =  0.04137532666374467 train_accur =  0.9986979166666666 test_accur =  0.8406004392048081\n",
      "Epoch :  175 training_loss =  0.04104615797806238 train_accur =  0.9986979166666666 test_accur =  0.8406148867313916\n",
      "Epoch :  176 training_loss =  0.04072393211484073 train_accur =  0.9986979166666666 test_accur =  0.8407738095238095\n",
      "Epoch :  177 training_loss =  0.04039621962694412 train_accur =  0.9986979166666666 test_accur =  0.8407593619972261\n",
      "Epoch :  178 training_loss =  0.04006782849361182 train_accur =  0.9986979166666666 test_accur =  0.8406726768377254\n",
      "Epoch :  179 training_loss =  0.039602330537492014 train_accur =  0.9986979166666666 test_accur =  0.840658229311142\n",
      "Epoch :  180 training_loss =  0.03925750173860416 train_accur =  0.9986979166666666 test_accur =  0.8406148867313916\n",
      "Epoch :  181 training_loss =  0.03896240998503086 train_accur =  0.9986979166666666 test_accur =  0.8407015718908922\n",
      "Epoch :  182 training_loss =  0.03866312141947535 train_accur =  0.9986979166666666 test_accur =  0.8409905224225612\n",
      "Epoch :  183 training_loss =  0.03833625837384113 train_accur =  0.9986979166666666 test_accur =  0.8409905224225612\n",
      "Epoch :  184 training_loss =  0.03799071623116071 train_accur =  0.9986979166666666 test_accur =  0.8411205501618123\n",
      "Epoch :  185 training_loss =  0.03771815232094799 train_accur =  0.9986979166666666 test_accur =  0.8408604946833103\n",
      "Epoch :  186 training_loss =  0.037427326431081535 train_accur =  0.9986979166666666 test_accur =  0.8409471798428109\n",
      "Epoch :  187 training_loss =  0.037157504655818324 train_accur =  0.9986979166666666 test_accur =  0.8412361303744799\n",
      "Epoch :  188 training_loss =  0.03688773200662562 train_accur =  0.9986979166666666 test_accur =  0.841337263060564\n",
      "Epoch :  189 training_loss =  0.0370332194820866 train_accur =  0.9986979166666666 test_accur =  0.8415106333795654\n",
      "Epoch :  190 training_loss =  0.036377115470183685 train_accur =  0.9986979166666666 test_accur =  0.8412505779010634\n",
      "Epoch :  191 training_loss =  0.036107935145504574 train_accur =  0.9986979166666666 test_accur =  0.841337263060564\n",
      "Epoch :  192 training_loss =  0.03584066954887577 train_accur =  0.9986979166666666 test_accur =  0.8412650254276468\n",
      "Epoch :  193 training_loss =  0.0355823186486076 train_accur =  0.9986979166666666 test_accur =  0.8413806056403144\n",
      "Epoch :  194 training_loss =  0.035330878050684134 train_accur =  0.9986979166666666 test_accur =  0.8413950531668978\n",
      "Epoch :  195 training_loss =  0.0354378871678416 train_accur =  0.9986979166666666 test_accur =  0.841207235321313\n",
      "Epoch :  196 training_loss =  0.03484132796557046 train_accur =  0.9986979166666666 test_accur =  0.8411638927415627\n",
      "Epoch :  197 training_loss =  0.03453899254856589 train_accur =  0.9986979166666666 test_accur =  0.8412216828478964\n",
      "Epoch :  198 training_loss =  0.03427496140810714 train_accur =  0.9986979166666666 test_accur =  0.8412361303744799\n",
      "Epoch :  199 training_loss =  0.034026708487929325 train_accur =  0.9986979166666666 test_accur =  0.841207235321313\n",
      "Epoch :  200 training_loss =  0.03376528536009808 train_accur =  0.9986979166666666 test_accur =  0.841077207582062\n",
      "Epoch :  201 training_loss =  0.033661929130119235 train_accur =  0.9986979166666666 test_accur =  0.8411783402681461\n",
      "Epoch :  202 training_loss =  0.03329527096415571 train_accur =  0.9986979166666666 test_accur =  0.8414239482200647\n",
      "Epoch :  203 training_loss =  0.03304856079461475 train_accur =  0.9986979166666666 test_accur =  0.8414528432732317\n",
      "Epoch :  204 training_loss =  0.03282951139212003 train_accur =  0.9986979166666666 test_accur =  0.8412794729542302\n",
      "Epoch :  205 training_loss =  0.03260971512612592 train_accur =  0.9986979166666666 test_accur =  0.841207235321313\n",
      "Epoch :  206 training_loss =  0.032391830224765064 train_accur =  0.9986979166666666 test_accur =  0.8412650254276468\n",
      "Epoch :  207 training_loss =  0.03217574838710266 train_accur =  0.9986979166666666 test_accur =  0.8412939204808136\n",
      "Epoch :  208 training_loss =  0.03192857726820701 train_accur =  0.9986979166666666 test_accur =  0.841207235321313\n",
      "Epoch :  209 training_loss =  0.03161669863862977 train_accur =  0.9986979166666666 test_accur =  0.8410916551086454\n",
      "Epoch :  210 training_loss =  0.031413467537278986 train_accur =  0.9986979166666666 test_accur =  0.841048312528895\n",
      "Epoch :  211 training_loss =  0.031215504433083983 train_accur =  0.9986979166666666 test_accur =  0.8410194174757282\n",
      "Epoch :  212 training_loss =  0.031023115622539198 train_accur =  0.9986979166666666 test_accur =  0.8412505779010634\n",
      "Epoch :  213 training_loss =  0.030801666825369833 train_accur =  0.9986979166666666 test_accur =  0.8413950531668978\n",
      "Epoch :  214 training_loss =  0.030598248571507308 train_accur =  0.9986979166666666 test_accur =  0.8414239482200647\n",
      "Epoch :  215 training_loss =  0.030397047743820942 train_accur =  0.9986979166666666 test_accur =  0.8414528432732317\n",
      "Epoch :  216 training_loss =  0.030199250856808846 train_accur =  0.9986979166666666 test_accur =  0.8415250809061489\n",
      "Epoch :  217 training_loss =  0.030133828733479157 train_accur =  0.9986979166666666 test_accur =  0.8417562413314841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  218 training_loss =  0.02979762031087293 train_accur =  0.9986979166666666 test_accur =  0.8416406611188164\n",
      "Epoch :  219 training_loss =  0.02955447042183212 train_accur =  0.9986979166666666 test_accur =  0.841626213592233\n",
      "Epoch :  220 training_loss =  0.02935792185220666 train_accur =  0.9986979166666666 test_accur =  0.8416984512251503\n",
      "Epoch :  221 training_loss =  0.029158300433751904 train_accur =  0.9986979166666666 test_accur =  0.8417562413314841\n",
      "Epoch :  222 training_loss =  0.028971198152364055 train_accur =  0.9986979166666666 test_accur =  0.8418284789644013\n",
      "Epoch :  223 training_loss =  0.028787913565808287 train_accur =  0.9986979166666666 test_accur =  0.8419296116504854\n",
      "Epoch :  224 training_loss =  0.028609662604047188 train_accur =  0.9986979166666666 test_accur =  0.8420162968099861\n",
      "Epoch :  225 training_loss =  0.028434611386534657 train_accur =  0.9986979166666666 test_accur =  0.8420596393897365\n",
      "Epoch :  226 training_loss =  0.028260757772075706 train_accur =  0.9986979166666666 test_accur =  0.8420162968099861\n",
      "Epoch :  227 training_loss =  0.028087280836289787 train_accur =  0.9986979166666666 test_accur =  0.8421029819694869\n",
      "Epoch :  228 training_loss =  0.02791585498002746 train_accur =  0.9986979166666666 test_accur =  0.842204114655571\n",
      "Epoch :  229 training_loss =  0.02774541583612492 train_accur =  0.9986979166666666 test_accur =  0.8422907998150717\n",
      "Epoch :  230 training_loss =  0.027571413846207385 train_accur =  0.9986979166666666 test_accur =  0.8422474572353214\n",
      "Epoch :  231 training_loss =  0.027399370279379335 train_accur =  0.9986979166666666 test_accur =  0.8422907998150717\n",
      "Epoch :  232 training_loss =  0.027237281156941583 train_accur =  0.9986979166666666 test_accur =  0.8422619047619048\n",
      "Epoch :  233 training_loss =  0.02713020062825834 train_accur =  0.9986979166666666 test_accur =  0.842175219602404\n",
      "Epoch :  234 training_loss =  0.026906196956219645 train_accur =  0.9986979166666666 test_accur =  0.8420162968099861\n",
      "Epoch :  235 training_loss =  0.02674685880097324 train_accur =  0.9986979166666666 test_accur =  0.8421896671289875\n",
      "Epoch :  236 training_loss =  0.02660573226758514 train_accur =  0.9986979166666666 test_accur =  0.8422474572353214\n",
      "Epoch :  237 training_loss =  0.026430092249723924 train_accur =  0.9986979166666666 test_accur =  0.8421896671289875\n",
      "Epoch :  238 training_loss =  0.026275213820745366 train_accur =  0.9986979166666666 test_accur =  0.8422185621821544\n",
      "Epoch :  239 training_loss =  0.02612323905310149 train_accur =  0.9986979166666666 test_accur =  0.8422330097087378\n",
      "Epoch :  240 training_loss =  0.025972855973612194 train_accur =  0.9986979166666666 test_accur =  0.842334142394822\n",
      "Epoch :  241 training_loss =  0.025824345418416876 train_accur =  0.9986979166666666 test_accur =  0.8422763522884882\n",
      "Epoch :  242 training_loss =  0.02567793403539676 train_accur =  0.9986979166666666 test_accur =  0.8423774849745723\n",
      "Epoch :  243 training_loss =  0.025529364150551917 train_accur =  0.9986979166666666 test_accur =  0.8424208275543227\n",
      "Epoch :  244 training_loss =  0.025375542221802855 train_accur =  0.9986979166666666 test_accur =  0.8425075127138234\n",
      "Epoch :  245 training_loss =  0.025177819554828014 train_accur =  0.9986979166666666 test_accur =  0.8426375404530745\n",
      "Epoch :  246 training_loss =  0.02541023944177058 train_accur =  0.9986979166666666 test_accur =  0.8428687008784096\n",
      "Epoch :  247 training_loss =  0.02488607860162178 train_accur =  0.9986979166666666 test_accur =  0.8428687008784096\n",
      "Epoch :  248 training_loss =  0.024723904440663302 train_accur =  0.9986979166666666 test_accur =  0.8429264909847434\n",
      "Epoch :  249 training_loss =  0.024591107764179422 train_accur =  0.9986979166666666 test_accur =  0.8428975959315765\n",
      "Epoch :  250 training_loss =  0.024458163087795054 train_accur =  0.9986979166666666 test_accur =  0.8430565187239945\n",
      "Epoch :  251 training_loss =  0.0243267488419244 train_accur =  0.9986979166666666 test_accur =  0.8431143088303282\n",
      "Epoch :  252 training_loss =  0.02419611291459279 train_accur =  0.9986979166666666 test_accur =  0.843172098936662\n",
      "Epoch :  253 training_loss =  0.024063761381226137 train_accur =  0.9986979166666666 test_accur =  0.8431865464632455\n",
      "Epoch :  254 training_loss =  0.023934493529982344 train_accur =  0.9986979166666666 test_accur =  0.8432298890429958\n",
      "Epoch :  255 training_loss =  0.023809540203668662 train_accur =  0.9986979166666666 test_accur =  0.8434321544151642\n",
      "Epoch :  256 training_loss =  0.02368851047569284 train_accur =  0.9986979166666666 test_accur =  0.8434754969949144\n",
      "Epoch :  257 training_loss =  0.023554944722205845 train_accur =  0.9986979166666666 test_accur =  0.8432443365695793\n",
      "Epoch :  258 training_loss =  0.023427976523642473 train_accur =  0.9986979166666666 test_accur =  0.8432443365695793\n",
      "Epoch :  259 training_loss =  0.023299295148561893 train_accur =  0.9986979166666666 test_accur =  0.8432587840961627\n",
      "Epoch :  260 training_loss =  0.023169868791412834 train_accur =  0.9986979166666666 test_accur =  0.8433165742024965\n",
      "Epoch :  261 training_loss =  0.023042060024933666 train_accur =  0.9986979166666666 test_accur =  0.843172098936662\n",
      "Epoch :  262 training_loss =  0.022914257524663254 train_accur =  0.9986979166666666 test_accur =  0.843200993989829\n",
      "Epoch :  263 training_loss =  0.02278715855649971 train_accur =  0.9986979166666666 test_accur =  0.8432443365695793\n",
      "Epoch :  264 training_loss =  0.02265779214606782 train_accur =  0.9986979166666666 test_accur =  0.8434754969949144\n",
      "Epoch :  265 training_loss =  0.022532597001725448 train_accur =  0.9986979166666666 test_accur =  0.8435043920480814\n",
      "Epoch :  266 training_loss =  0.022410116772973596 train_accur =  1.0 test_accur =  0.8433888118354138\n",
      "Epoch :  267 training_loss =  0.02229105971787402 train_accur =  1.0 test_accur =  0.8433599167822469\n",
      "Epoch :  268 training_loss =  0.022174205581040423 train_accur =  1.0 test_accur =  0.84333102172908\n",
      "Epoch :  269 training_loss =  0.022058055074226843 train_accur =  1.0 test_accur =  0.8432587840961627\n",
      "Epoch :  270 training_loss =  0.021941275130898212 train_accur =  1.0 test_accur =  0.8433165742024965\n",
      "Epoch :  271 training_loss =  0.02181975602321969 train_accur =  1.0 test_accur =  0.8433454692556634\n",
      "Epoch :  272 training_loss =  0.021694216062925188 train_accur =  1.0 test_accur =  0.8433599167822469\n",
      "Epoch :  273 training_loss =  0.021580519600731433 train_accur =  1.0 test_accur =  0.8434032593619972\n",
      "Epoch :  274 training_loss =  0.021470537882765636 train_accur =  1.0 test_accur =  0.8434321544151642\n",
      "Epoch :  275 training_loss =  0.021361876270627148 train_accur =  1.0 test_accur =  0.8435188395746648\n",
      "Epoch :  276 training_loss =  0.02125473669528726 train_accur =  1.0 test_accur =  0.8435477346278317\n",
      "Epoch :  277 training_loss =  0.021149331129904046 train_accur =  1.0 test_accur =  0.8435043920480814\n",
      "Epoch :  278 training_loss =  0.021045295312559435 train_accur =  1.0 test_accur =  0.8435332871012483\n",
      "Epoch :  279 training_loss =  0.020941835720564975 train_accur =  1.0 test_accur =  0.8436344197873324\n",
      "Epoch :  280 training_loss =  0.020839497984827718 train_accur =  1.0 test_accur =  0.8436777623670828\n",
      "Epoch :  281 training_loss =  0.020738799533039674 train_accur =  1.0 test_accur =  0.8435621821544151\n",
      "Epoch :  282 training_loss =  0.02064371627216901 train_accur =  1.0 test_accur =  0.8435188395746648\n",
      "Epoch :  283 training_loss =  0.020550531855383473 train_accur =  1.0 test_accur =  0.8435621821544151\n",
      "Epoch :  284 training_loss =  0.020462694936365336 train_accur =  1.0 test_accur =  0.8435332871012483\n",
      "Epoch :  285 training_loss =  0.0203693750336612 train_accur =  1.0 test_accur =  0.843489944521498\n",
      "Epoch :  286 training_loss =  0.020259173899617684 train_accur =  1.0 test_accur =  0.8435043920480814\n",
      "Epoch :  287 training_loss =  0.020161703263710508 train_accur =  1.0 test_accur =  0.8435188395746648\n",
      "Epoch :  288 training_loss =  0.020066106282838524 train_accur =  1.0 test_accur =  0.8435766296809986\n",
      "Epoch :  289 training_loss =  0.019974291477701602 train_accur =  1.0 test_accur =  0.8436922098936662\n",
      "Epoch :  290 training_loss =  0.019878802852391686 train_accur =  1.0 test_accur =  0.8436922098936662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  291 training_loss =  0.019783760318498503 train_accur =  1.0 test_accur =  0.8435766296809986\n",
      "Epoch :  292 training_loss =  0.01968950687403788 train_accur =  1.0 test_accur =  0.8435188395746648\n",
      "Epoch :  293 training_loss =  0.019596555686779968 train_accur =  1.0 test_accur =  0.8435188395746648\n",
      "Epoch :  294 training_loss =  0.019504651620923172 train_accur =  1.0 test_accur =  0.8435332871012483\n",
      "Epoch :  295 training_loss =  0.019413551296870585 train_accur =  1.0 test_accur =  0.8435621821544151\n",
      "Epoch :  296 training_loss =  0.019322653142479773 train_accur =  1.0 test_accur =  0.8435766296809986\n",
      "Epoch :  297 training_loss =  0.019235269243582123 train_accur =  1.0 test_accur =  0.8436488673139159\n",
      "Epoch :  298 training_loss =  0.01914673942449936 train_accur =  1.0 test_accur =  0.8437066574202496\n",
      "Epoch :  299 training_loss =  0.019058650610262608 train_accur =  1.0 test_accur =  0.8437211049468331\n",
      "Epoch :  300 training_loss =  0.018961189550055935 train_accur =  1.0 test_accur =  0.8437644475265834\n",
      "Epoch :  301 training_loss =  0.01885644745532642 train_accur =  1.0 test_accur =  0.8439089227924179\n",
      "Epoch :  302 training_loss =  0.0187626284116302 train_accur =  1.0 test_accur =  0.8439811604253352\n",
      "Epoch :  303 training_loss =  0.018679280136106785 train_accur =  1.0 test_accur =  0.8440967406380028\n",
      "Epoch :  304 training_loss =  0.018598144617269358 train_accur =  1.0 test_accur =  0.8441400832177531\n",
      "Epoch :  305 training_loss =  0.018525155343125717 train_accur =  1.0 test_accur =  0.8441256356911697\n",
      "Epoch :  306 training_loss =  0.018437822254232027 train_accur =  1.0 test_accur =  0.844299006010171\n",
      "Epoch :  307 training_loss =  0.01835369700032758 train_accur =  1.0 test_accur =  0.8443134535367545\n",
      "Epoch :  308 training_loss =  0.01827071839837587 train_accur =  1.0 test_accur =  0.8442701109570042\n",
      "Epoch :  309 training_loss =  0.01818847988793311 train_accur =  1.0 test_accur =  0.8442701109570042\n",
      "Epoch :  310 training_loss =  0.01810241113684819 train_accur =  1.0 test_accur =  0.844299006010171\n",
      "Epoch :  311 training_loss =  0.018011585657039968 train_accur =  1.0 test_accur =  0.8444001386962552\n",
      "Epoch :  312 training_loss =  0.017919313981961477 train_accur =  1.0 test_accur =  0.8444868238557559\n",
      "Epoch :  313 training_loss =  0.01783746671150324 train_accur =  1.0 test_accur =  0.8445012713823393\n",
      "Epoch :  314 training_loss =  0.017757386872134785 train_accur =  1.0 test_accur =  0.8445590614886731\n",
      "Epoch :  315 training_loss =  0.01767656333661518 train_accur =  1.0 test_accur =  0.8446601941747572\n",
      "Epoch :  316 training_loss =  0.017592575288495185 train_accur =  1.0 test_accur =  0.8447613268608414\n",
      "Epoch :  317 training_loss =  0.017501330358914217 train_accur =  1.0 test_accur =  0.844876907073509\n",
      "Epoch :  318 training_loss =  0.017401111384125895 train_accur =  1.0 test_accur =  0.8449491447064262\n",
      "Epoch :  319 training_loss =  0.017309434629553287 train_accur =  1.0 test_accur =  0.8449635922330098\n",
      "Epoch :  320 training_loss =  0.017231317346889017 train_accur =  1.0 test_accur =  0.8449924872861766\n",
      "Epoch :  321 training_loss =  0.017152430197789545 train_accur =  1.0 test_accur =  0.8449635922330098\n",
      "Epoch :  322 training_loss =  0.017067808239979214 train_accur =  1.0 test_accur =  0.8450502773925104\n",
      "Epoch :  323 training_loss =  0.01698553350407446 train_accur =  1.0 test_accur =  0.8449924872861766\n",
      "Epoch :  324 training_loss =  0.016912575494595225 train_accur =  1.0 test_accur =  0.84500693481276\n",
      "Epoch :  325 training_loss =  0.01684153729263588 train_accur =  1.0 test_accur =  0.8449635922330098\n",
      "Epoch :  326 training_loss =  0.016771209488151657 train_accur =  1.0 test_accur =  0.84500693481276\n",
      "Epoch :  327 training_loss =  0.016701390300623623 train_accur =  1.0 test_accur =  0.8450502773925104\n",
      "Epoch :  328 training_loss =  0.016632031522030917 train_accur =  1.0 test_accur =  0.8450647249190939\n",
      "Epoch :  329 training_loss =  0.016563096943436308 train_accur =  1.0 test_accur =  0.8450936199722607\n",
      "Epoch :  330 training_loss =  0.01649450548086369 train_accur =  1.0 test_accur =  0.8450791724456773\n",
      "Epoch :  331 training_loss =  0.016426336743405053 train_accur =  1.0 test_accur =  0.8450502773925104\n",
      "Epoch :  332 training_loss =  0.016358435218524314 train_accur =  1.0 test_accur =  0.8450647249190939\n",
      "Epoch :  333 training_loss =  0.016290138253797853 train_accur =  1.0 test_accur =  0.8451369625520111\n",
      "Epoch :  334 training_loss =  0.01621927706937426 train_accur =  1.0 test_accur =  0.845165857605178\n",
      "Epoch :  335 training_loss =  0.016141515726362306 train_accur =  1.0 test_accur =  0.8451803051317615\n",
      "Epoch :  336 training_loss =  0.016071450188117047 train_accur =  1.0 test_accur =  0.8452380952380952\n",
      "Epoch :  337 training_loss =  0.01600450640101656 train_accur =  1.0 test_accur =  0.845295885344429\n",
      "Epoch :  338 training_loss =  0.015936392084604345 train_accur =  1.0 test_accur =  0.8453103328710125\n",
      "Epoch :  339 training_loss =  0.015865468357235138 train_accur =  1.0 test_accur =  0.8452236477115118\n",
      "Epoch :  340 training_loss =  0.015798985099219332 train_accur =  1.0 test_accur =  0.8453103328710125\n",
      "Epoch :  341 training_loss =  0.015734737155955202 train_accur =  1.0 test_accur =  0.8453392279241794\n",
      "Epoch :  342 training_loss =  0.015670415652491596 train_accur =  1.0 test_accur =  0.8453536754507628\n",
      "Epoch :  343 training_loss =  0.015606651272565678 train_accur =  1.0 test_accur =  0.8453825705039297\n",
      "Epoch :  344 training_loss =  0.015543067940320898 train_accur =  1.0 test_accur =  0.8454114655570967\n",
      "Epoch :  345 training_loss =  0.015480348346924713 train_accur =  1.0 test_accur =  0.8454548081368469\n",
      "Epoch :  346 training_loss =  0.015418401960023132 train_accur =  1.0 test_accur =  0.8454837031900139\n",
      "Epoch :  347 training_loss =  0.015357144304233227 train_accur =  1.0 test_accur =  0.8455125982431808\n",
      "Epoch :  348 training_loss =  0.015296452368122927 train_accur =  1.0 test_accur =  0.8454981507165973\n",
      "Epoch :  349 training_loss =  0.015236113321995974 train_accur =  1.0 test_accur =  0.8455270457697642\n",
      "Epoch :  350 training_loss =  0.015176689909448534 train_accur =  1.0 test_accur =  0.8455559408229311\n",
      "Epoch :  351 training_loss =  0.01512145874909995 train_accur =  1.0 test_accur =  0.8455559408229311\n",
      "Epoch :  352 training_loss =  0.015058726495637748 train_accur =  1.0 test_accur =  0.8455414932963476\n",
      "Epoch :  353 training_loss =  0.014999272254635822 train_accur =  1.0 test_accur =  0.8456570735090153\n",
      "Epoch :  354 training_loss =  0.014937640932752076 train_accur =  1.0 test_accur =  0.8457293111419325\n",
      "Epoch :  355 training_loss =  0.014874464624073285 train_accur =  1.0 test_accur =  0.8457293111419325\n",
      "Epoch :  356 training_loss =  0.014807811767045969 train_accur =  1.0 test_accur =  0.8457148636153491\n",
      "Epoch :  357 training_loss =  0.0147414223758085 train_accur =  1.0 test_accur =  0.8457148636153491\n",
      "Epoch :  358 training_loss =  0.014679260493895362 train_accur =  1.0 test_accur =  0.8457004160887656\n",
      "Epoch :  359 training_loss =  0.014620691396613313 train_accur =  1.0 test_accur =  0.8456859685621821\n",
      "Epoch :  360 training_loss =  0.01456366911743829 train_accur =  1.0 test_accur =  0.8457004160887656\n",
      "Epoch :  361 training_loss =  0.014507985573391792 train_accur =  1.0 test_accur =  0.8456715210355987\n",
      "Epoch :  362 training_loss =  0.014453305025760973 train_accur =  1.0 test_accur =  0.8456715210355987\n",
      "Epoch :  363 training_loss =  0.01439942731813506 train_accur =  1.0 test_accur =  0.8454981507165973\n",
      "Epoch :  364 training_loss =  0.014346007697273074 train_accur =  1.0 test_accur =  0.8454981507165973\n",
      "Epoch :  365 training_loss =  0.01429290838654873 train_accur =  1.0 test_accur =  0.8454981507165973\n",
      "Epoch :  366 training_loss =  0.014240733832885322 train_accur =  1.0 test_accur =  0.8454548081368469\n",
      "Epoch :  367 training_loss =  0.014188990695255366 train_accur =  1.0 test_accur =  0.8454403606102635\n",
      "Epoch :  368 training_loss =  0.014137416294923974 train_accur =  1.0 test_accur =  0.8454259130836801\n",
      "Epoch :  369 training_loss =  0.014086083031059755 train_accur =  1.0 test_accur =  0.8453392279241794\n",
      "Epoch :  370 training_loss =  0.014034890486998394 train_accur =  1.0 test_accur =  0.8453536754507628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  371 training_loss =  0.013984622593213453 train_accur =  1.0 test_accur =  0.845295885344429\n",
      "Epoch :  372 training_loss =  0.013937064696265518 train_accur =  1.0 test_accur =  0.8452814378178456\n",
      "Epoch :  373 training_loss =  0.013884789028639159 train_accur =  1.0 test_accur =  0.8453536754507628\n",
      "Epoch :  374 training_loss =  0.013834327111385455 train_accur =  1.0 test_accur =  0.8454259130836801\n",
      "Epoch :  375 training_loss =  0.013784645607644887 train_accur =  1.0 test_accur =  0.8454548081368469\n",
      "Epoch :  376 training_loss =  0.013735462245242535 train_accur =  1.0 test_accur =  0.8454403606102635\n",
      "Epoch :  377 training_loss =  0.013686579926143822 train_accur =  1.0 test_accur =  0.8454403606102635\n",
      "Epoch :  378 training_loss =  0.013637952729568832 train_accur =  1.0 test_accur =  0.8454403606102635\n",
      "Epoch :  379 training_loss =  0.01358954028170153 train_accur =  1.0 test_accur =  0.8454692556634305\n",
      "Epoch :  380 training_loss =  0.013541275420408902 train_accur =  1.0 test_accur =  0.8454403606102635\n",
      "Epoch :  381 training_loss =  0.013493105716634915 train_accur =  1.0 test_accur =  0.8453825705039297\n",
      "Epoch :  382 training_loss =  0.013445076709724741 train_accur =  1.0 test_accur =  0.8453970180305131\n",
      "Epoch :  383 training_loss =  0.013397274107530655 train_accur =  1.0 test_accur =  0.8453681229773463\n",
      "Epoch :  384 training_loss =  0.013349625186999255 train_accur =  1.0 test_accur =  0.8453392279241794\n",
      "Epoch :  385 training_loss =  0.013301904707273132 train_accur =  1.0 test_accur =  0.8453392279241794\n",
      "Epoch :  386 training_loss =  0.013253875831066866 train_accur =  1.0 test_accur =  0.845324780397596\n",
      "Epoch :  387 training_loss =  0.013205584845276461 train_accur =  1.0 test_accur =  0.8453536754507628\n",
      "Epoch :  388 training_loss =  0.013157789188214607 train_accur =  1.0 test_accur =  0.845295885344429\n",
      "Epoch :  389 training_loss =  0.01311109628946827 train_accur =  1.0 test_accur =  0.8453825705039297\n",
      "Epoch :  390 training_loss =  0.013065234543704746 train_accur =  1.0 test_accur =  0.8454981507165973\n",
      "Epoch :  391 training_loss =  0.013019861847285064 train_accur =  1.0 test_accur =  0.8455559408229311\n",
      "Epoch :  392 training_loss =  0.012975066063953537 train_accur =  1.0 test_accur =  0.845584835876098\n",
      "Epoch :  393 training_loss =  0.012930723206740843 train_accur =  1.0 test_accur =  0.8455992834026814\n",
      "Epoch :  394 training_loss =  0.012886628393769563 train_accur =  1.0 test_accur =  0.8455992834026814\n",
      "Epoch :  395 training_loss =  0.01284272313821238 train_accur =  1.0 test_accur =  0.8456426259824318\n",
      "Epoch :  396 training_loss =  0.012798910784613912 train_accur =  1.0 test_accur =  0.8456715210355987\n",
      "Epoch :  397 training_loss =  0.01275516629567158 train_accur =  1.0 test_accur =  0.8456715210355987\n",
      "Epoch :  398 training_loss =  0.012711473844437802 train_accur =  1.0 test_accur =  0.8456859685621821\n",
      "Epoch :  399 training_loss =  0.012667856952245295 train_accur =  1.0 test_accur =  0.8456426259824318\n",
      "Epoch :  400 training_loss =  0.012624427610527018 train_accur =  1.0 test_accur =  0.8456281784558484\n",
      "Epoch :  401 training_loss =  0.012581318334052414 train_accur =  1.0 test_accur =  0.8455992834026814\n",
      "Epoch :  402 training_loss =  0.012538387915093012 train_accur =  1.0 test_accur =  0.8456715210355987\n",
      "Epoch :  403 training_loss =  0.012495712402598225 train_accur =  1.0 test_accur =  0.8456859685621821\n",
      "Epoch :  404 training_loss =  0.012453284982594666 train_accur =  1.0 test_accur =  0.8456859685621821\n",
      "Epoch :  405 training_loss =  0.012411093493901093 train_accur =  1.0 test_accur =  0.8457293111419325\n",
      "Epoch :  406 training_loss =  0.012369140188736185 train_accur =  1.0 test_accur =  0.8457582061950994\n",
      "Epoch :  407 training_loss =  0.012327433749674117 train_accur =  1.0 test_accur =  0.8457582061950994\n",
      "Epoch :  408 training_loss =  0.01228592986964752 train_accur =  1.0 test_accur =  0.8457871012482663\n",
      "Epoch :  409 training_loss =  0.01224450499496875 train_accur =  1.0 test_accur =  0.8458304438280166\n",
      "Epoch :  410 training_loss =  0.012202939365558416 train_accur =  1.0 test_accur =  0.8458448913546001\n",
      "Epoch :  411 training_loss =  0.012160962482705626 train_accur =  1.0 test_accur =  0.8458448913546001\n",
      "Epoch :  412 training_loss =  0.012116549700335603 train_accur =  1.0 test_accur =  0.8457437586685159\n",
      "Epoch :  413 training_loss =  0.012069326636558802 train_accur =  1.0 test_accur =  0.8456426259824318\n",
      "Epoch :  414 training_loss =  0.012024435935535621 train_accur =  1.0 test_accur =  0.8456281784558484\n",
      "Epoch :  415 training_loss =  0.011983080508876504 train_accur =  1.0 test_accur =  0.8458159963014332\n",
      "Epoch :  416 training_loss =  0.011942501578237768 train_accur =  1.0 test_accur =  0.845873786407767\n",
      "Epoch :  417 training_loss =  0.01190235484195835 train_accur =  1.0 test_accur =  0.8458882339343504\n",
      "Epoch :  418 training_loss =  0.011862428466193191 train_accur =  1.0 test_accur =  0.8459315765141008\n",
      "Epoch :  419 training_loss =  0.011823144361632402 train_accur =  1.0 test_accur =  0.8459893666204346\n",
      "Epoch :  420 training_loss =  0.011785487554391802 train_accur =  1.0 test_accur =  0.8460616042533519\n",
      "Epoch :  421 training_loss =  0.011748060109010095 train_accur =  1.0 test_accur =  0.8461193943596856\n",
      "Epoch :  422 training_loss =  0.011707337369187873 train_accur =  1.0 test_accur =  0.8462060795191864\n",
      "Epoch :  423 training_loss =  0.01166833610283349 train_accur =  1.0 test_accur =  0.8463505547850209\n",
      "Epoch :  424 training_loss =  0.011629158533593836 train_accur =  1.0 test_accur =  0.8464083448913546\n",
      "Epoch :  425 training_loss =  0.011589303531899258 train_accur =  1.0 test_accur =  0.8464372399445215\n",
      "Epoch :  426 training_loss =  0.01154868035459044 train_accur =  1.0 test_accur =  0.8464805825242718\n",
      "Epoch :  427 training_loss =  0.011507429052088921 train_accur =  1.0 test_accur =  0.8464805825242718\n",
      "Epoch :  428 training_loss =  0.011466781759479367 train_accur =  1.0 test_accur =  0.8464805825242718\n",
      "Epoch :  429 training_loss =  0.011427531115054984 train_accur =  1.0 test_accur =  0.8465239251040222\n",
      "Epoch :  430 training_loss =  0.011389111367264 train_accur =  1.0 test_accur =  0.8465383726306056\n",
      "Epoch :  431 training_loss =  0.011351605542005826 train_accur =  1.0 test_accur =  0.8465672676837726\n",
      "Epoch :  432 training_loss =  0.011314634577437662 train_accur =  1.0 test_accur =  0.8465961627369394\n",
      "Epoch :  433 training_loss =  0.011278255438599925 train_accur =  1.0 test_accur =  0.8466250577901063\n",
      "Epoch :  434 training_loss =  0.011242375620961092 train_accur =  1.0 test_accur =  0.8466539528432733\n",
      "Epoch :  435 training_loss =  0.0112068662277447 train_accur =  1.0 test_accur =  0.8466684003698567\n",
      "Epoch :  436 training_loss =  0.011171635062076787 train_accur =  1.0 test_accur =  0.8466972954230236\n",
      "Epoch :  437 training_loss =  0.011136604591438653 train_accur =  1.0 test_accur =  0.8467261904761905\n",
      "Epoch :  438 training_loss =  0.01110173471961461 train_accur =  1.0 test_accur =  0.8467695330559408\n",
      "Epoch :  439 training_loss =  0.011067042970959829 train_accur =  1.0 test_accur =  0.8468273231622746\n",
      "Epoch :  440 training_loss =  0.011032612216056831 train_accur =  1.0 test_accur =  0.8469429033749422\n",
      "Epoch :  441 training_loss =  0.010998582489129261 train_accur =  1.0 test_accur =  0.8470151410078595\n",
      "Epoch :  442 training_loss =  0.01096493966900889 train_accur =  1.0 test_accur =  0.847130721220527\n",
      "Epoch :  443 training_loss =  0.010931381744019324 train_accur =  1.0 test_accur =  0.8472318539066112\n",
      "Epoch :  444 training_loss =  0.010897948186281191 train_accur =  1.0 test_accur =  0.8472174063800277\n",
      "Epoch :  445 training_loss =  0.01086465170608011 train_accur =  1.0 test_accur =  0.8472318539066112\n",
      "Epoch :  446 training_loss =  0.010831432667119395 train_accur =  1.0 test_accur =  0.8472463014331947\n",
      "Epoch :  447 training_loss =  0.010798144722406687 train_accur =  1.0 test_accur =  0.8472463014331947\n",
      "Epoch :  448 training_loss =  0.010764426049839272 train_accur =  1.0 test_accur =  0.8472318539066112\n",
      "Epoch :  449 training_loss =  0.010729371629387844 train_accur =  1.0 test_accur =  0.8472174063800277\n",
      "Epoch :  450 training_loss =  0.01069197655803561 train_accur =  1.0 test_accur =  0.8471885113268608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  451 training_loss =  0.010655319342723835 train_accur =  1.0 test_accur =  0.8472029588534443\n",
      "Epoch :  452 training_loss =  0.01062187993926046 train_accur =  1.0 test_accur =  0.8471740638002774\n",
      "Epoch :  453 training_loss =  0.010589716975453713 train_accur =  1.0 test_accur =  0.8471162736939436\n",
      "Epoch :  454 training_loss =  0.010558085010833234 train_accur =  1.0 test_accur =  0.8470873786407767\n",
      "Epoch :  455 training_loss =  0.01052690326464513 train_accur =  1.0 test_accur =  0.8470440360610264\n",
      "Epoch :  456 training_loss =  0.010496477827858562 train_accur =  1.0 test_accur =  0.8469862459546925\n",
      "Epoch :  457 training_loss =  0.010466608538689571 train_accur =  1.0 test_accur =  0.8469862459546925\n",
      "Epoch :  458 training_loss =  0.01043682500945311 train_accur =  1.0 test_accur =  0.8469862459546925\n",
      "Epoch :  459 training_loss =  0.01040708444820234 train_accur =  1.0 test_accur =  0.8469429033749422\n",
      "Epoch :  460 training_loss =  0.010377367782372202 train_accur =  1.0 test_accur =  0.8468995607951919\n",
      "Epoch :  461 training_loss =  0.01034733467576698 train_accur =  1.0 test_accur =  0.8470584835876098\n",
      "Epoch :  462 training_loss =  0.01031436560359952 train_accur =  1.0 test_accur =  0.8470873786407767\n",
      "Epoch :  463 training_loss =  0.010282121507307205 train_accur =  1.0 test_accur =  0.8470584835876098\n",
      "Epoch :  464 training_loss =  0.01025117983098946 train_accur =  1.0 test_accur =  0.8469717984281091\n",
      "Epoch :  465 training_loss =  0.010220985096623628 train_accur =  1.0 test_accur =  0.8469573509015257\n",
      "Epoch :  466 training_loss =  0.010191286308882597 train_accur =  1.0 test_accur =  0.8469717984281091\n",
      "Epoch :  467 training_loss =  0.010161860085502347 train_accur =  1.0 test_accur =  0.8469862459546925\n",
      "Epoch :  468 training_loss =  0.010132644119592322 train_accur =  1.0 test_accur =  0.8469573509015257\n",
      "Epoch :  469 training_loss =  0.010103638076444552 train_accur =  1.0 test_accur =  0.8469284558483587\n",
      "Epoch :  470 training_loss =  0.01007482718106028 train_accur =  1.0 test_accur =  0.8469717984281091\n",
      "Epoch :  471 training_loss =  0.010046197124099984 train_accur =  1.0 test_accur =  0.8469717984281091\n",
      "Epoch :  472 training_loss =  0.010017736423191315 train_accur =  1.0 test_accur =  0.8469717984281091\n",
      "Epoch :  473 training_loss =  0.009989436801535322 train_accur =  1.0 test_accur =  0.8470151410078595\n",
      "Epoch :  474 training_loss =  0.009961293194129254 train_accur =  1.0 test_accur =  0.8470440360610264\n",
      "Epoch :  475 training_loss =  0.00993330350365432 train_accur =  1.0 test_accur =  0.8470440360610264\n",
      "Epoch :  476 training_loss =  0.009905468306675309 train_accur =  1.0 test_accur =  0.8470440360610264\n",
      "Epoch :  477 training_loss =  0.00987778955092286 train_accur =  1.0 test_accur =  0.8470295885344429\n",
      "Epoch :  478 training_loss =  0.00985026622034719 train_accur =  1.0 test_accur =  0.8470295885344429\n",
      "Epoch :  479 training_loss =  0.009822895999337809 train_accur =  1.0 test_accur =  0.8470295885344429\n",
      "Epoch :  480 training_loss =  0.009795683329628463 train_accur =  1.0 test_accur =  0.8470440360610264\n",
      "Epoch :  481 training_loss =  0.009768631718479286 train_accur =  1.0 test_accur =  0.8470729311141932\n",
      "Epoch :  482 training_loss =  0.00974172310812035 train_accur =  1.0 test_accur =  0.8471162736939436\n",
      "Epoch :  483 training_loss =  0.009714929749642037 train_accur =  1.0 test_accur =  0.8472318539066112\n",
      "Epoch :  484 training_loss =  0.00968825619303304 train_accur =  1.0 test_accur =  0.8472463014331947\n",
      "Epoch :  485 training_loss =  0.00966177792716949 train_accur =  1.0 test_accur =  0.8472751964863615\n",
      "Epoch :  486 training_loss =  0.009635982131614594 train_accur =  1.0 test_accur =  0.8473618816458622\n",
      "Epoch :  487 training_loss =  0.009609378134530257 train_accur =  1.0 test_accur =  0.8474774618585298\n",
      "Epoch :  488 training_loss =  0.009582971658448578 train_accur =  1.0 test_accur =  0.8475352519648636\n",
      "Epoch :  489 training_loss =  0.009556702480065949 train_accur =  1.0 test_accur =  0.8475641470180305\n",
      "Epoch :  490 training_loss =  0.009530560171301796 train_accur =  1.0 test_accur =  0.8475785945446139\n",
      "Epoch :  491 training_loss =  0.009504550211899162 train_accur =  1.0 test_accur =  0.8475930420711975\n",
      "Epoch :  492 training_loss =  0.009478668491999034 train_accur =  1.0 test_accur =  0.8476074895977809\n",
      "Epoch :  493 training_loss =  0.009452883964066702 train_accur =  1.0 test_accur =  0.8476219371243643\n",
      "Epoch :  494 training_loss =  0.009427187492844599 train_accur =  1.0 test_accur =  0.8476074895977809\n",
      "Epoch :  495 training_loss =  0.009401684517411916 train_accur =  1.0 test_accur =  0.8475930420711975\n",
      "Epoch :  496 training_loss =  0.009376353953827226 train_accur =  1.0 test_accur =  0.8475641470180305\n",
      "Epoch :  497 training_loss =  0.009351166393141845 train_accur =  1.0 test_accur =  0.8475641470180305\n",
      "Epoch :  498 training_loss =  0.009326107605607627 train_accur =  1.0 test_accur =  0.8475208044382802\n",
      "Epoch :  499 training_loss =  0.009301170205664868 train_accur =  1.0 test_accur =  0.8475208044382802\n",
      "Epoch :  500 training_loss =  0.009276350021880514 train_accur =  1.0 test_accur =  0.8474919093851133\n"
     ]
    }
   ],
   "source": [
    "SGD_train_epoch(X_train, Y_train, batch_size = 64, epoch = 500, learning_rate = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\"W1\": np.random.randn(400, 784) * np.sqrt(1. / 784),\n",
    "#               \"b1\": np.zeros((400, 1)) * np.sqrt(1. / 784),\n",
    "#               \"W2\": np.random.randn(400, 400) * np.sqrt(1. / 400),\n",
    "#               \"b2\": np.zeros((400, 1)) * np.sqrt(1. / 400),\n",
    "#               \"W3\": np.random.randn(digits, 400) * np.sqrt(1. / 400),\n",
    "#               \"b3\": np.zeros((digits, 1)) * np.sqrt(1. / 400)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward(X, parameters):\n",
    "#     inoutput = {}\n",
    "#     inoutput[\"hiddenlayer1_output_temp\"] = np.matmul(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
    "#     inoutput[\"hiddenlayer1_output\"] = sigmoid(inoutput[\"hiddenlayer1_output_temp\"])\n",
    "    \n",
    "#     inoutput[\"hiddenlayer2_output_temp\"] = np.matmul(parameters[\"W2\"], inoutput[\"hiddenlayer1_output\"]) + parameters[\"b2\"]\n",
    "#     inoutput[\"hiddenlayer2_output\"] = sigmoid(inoutput[\"hiddenlayer2_output_temp\"])\n",
    "    \n",
    "#     inoutput[\"outputlayer_output_temp\"] = np.matmul(parameters[\"W3\"], inoutput[\"hiddenlayer2_output\"]) + parameters[\"b3\"]\n",
    "#     inoutput[\"outputlayer_output\"] = softmax(inoutput[\"outputlayer_output_temp\"])\n",
    "#     #inoutput[\"outputlayer_output\"] = y_hat\n",
    "\n",
    "#     return inoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def back_propagation(X, Y, parameters, inoutput, m_batch):\n",
    "#     CE_gradient = inoutput[\"outputlayer_output\"] - Y\n",
    "\n",
    "#     W3_gradient = (1. / m_batch) * np.matmul(CE_gradient, inoutput[\"hiddenlayer2_output\"].T)\n",
    "#     b3_gradient = (1. / m_batch) * np.sum(CE_gradient, axis=1, keepdims=True)\n",
    "\n",
    "#     # ---\n",
    "\n",
    "#     outputlayer_backward_output = np.matmul(parameters[\"W3\"].T, CE_gradient)\n",
    "#     hiddenlayer2_backward_input = outputlayer_backward_output * sigmoid_gradient(inoutput[\"hiddenlayer2_output_temp\"])\n",
    "\n",
    "#     W2_gradient = (1. / m_batch) * np.matmul(hiddenlayer2_backward_input, inoutput[\"hiddenlayer1_output\"].T)\n",
    "#     b2_gradient = (1. / m_batch) * np.sum(hiddenlayer2_backward_input, axis=1, keepdims=True)\n",
    "\n",
    "#     # ---\n",
    "\n",
    "#     hiddenlayer2_backward_output = np.matmul(parameters[\"W2\"].T, hiddenlayer2_backward_input)\n",
    "#     hiddenlayer1_backward_input = hiddenlayer2_backward_output * sigmoid_gradient(inoutput[\"hiddenlayer1_output_temp\"])\n",
    "\n",
    "#     W1_gradient = (1. / m_batch) * np.matmul(hiddenlayer1_backward_input, X.T)\n",
    "#     b1_gradient = (1. / m_batch) * np.sum(hiddenlayer1_backward_input, axis=1, keepdims=True)\n",
    "\n",
    "#     Wb_gradients = {\"W1_gradient\": W1_gradient, \"b1_gradient\": b1_gradient, \"W2_gradient\": W2_gradient,\n",
    "#                      \"b2_gradient\": b2_gradient, \"W3_gradient\": W3_gradient, \"b3_gradient\": b3_gradient}\n",
    "\n",
    "#     return Wb_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     epoch = 3\n",
    "#     batch_size = 64\n",
    "#     TrainError = []\n",
    "#     TestError = []\n",
    "#     for i in range(epoch):\n",
    "       \n",
    "#         # shuffle training set\n",
    "# #         permutation = np.random.permutation(X_train.shape[1])\n",
    "# #         X_train_shuffled = X_train[:, permutation]\n",
    "# #         Y_train_shuffled = Y_train[:, permutation]\n",
    "    \n",
    "#         batch_num = len(X_train) // batch_size\n",
    "#         predicts = []\n",
    "#         golds = []\n",
    "#         predicts_test = []\n",
    "#         golds_test = []\n",
    "#         learning_rate = 0.03\n",
    "        \n",
    "        \n",
    "#         for j in range(batch_num):\n",
    "#             begin = j * batch_size\n",
    "#             end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "#             X = X_train[:, begin:end]\n",
    "#             Y = Y_train[:, begin:end]\n",
    "#             m_batch = end - begin\n",
    "            \n",
    "#             inoutput = forward(X, parameters)\n",
    "#             Wb_gradients = back_propagation(X, Y, parameters, inoutput, m_batch)\n",
    "            \n",
    "#             W1_gradient = Wb_gradients[\"W1_gradient\"]\n",
    "#             b1_gradient = Wb_gradients[\"b1_gradient\"]\n",
    "#             W2_gradient = Wb_gradients[\"W2_gradient\"]\n",
    "#             b2_gradient = Wb_gradients[\"b2_gradient\"]\n",
    "#             W3_gradient = Wb_gradients[\"W3_gradient\"]\n",
    "#             b3_gradient = Wb_gradients[\"b3_gradient\"]\n",
    "            \n",
    "#             parameters[\"W1\"] = parameters[\"W1\"] - learning_rate * W1_gradient\n",
    "#             parameters[\"b1\"] = parameters[\"b1\"] - learning_rate * b1_gradient\n",
    "#             parameters[\"W2\"] = parameters[\"W2\"] - learning_rate * W2_gradient\n",
    "#             parameters[\"b2\"] = parameters[\"b2\"] - learning_rate * b2_gradient\n",
    "#             parameters[\"W3\"] = parameters[\"W3\"] - learning_rate * W3_gradient\n",
    "#             parameters[\"b3\"] = parameters[\"b3\"] - learning_rate * b3_gradient\n",
    "            \n",
    "            \n",
    "          \n",
    "#         inoutput = forward(X_train, parameters)\n",
    "#         #print(Y.shape)\n",
    "        \n",
    "#         #print(inoutput[\"outputlayer_output\"].shape)\n",
    "#         train_loss = cross_entropy(Y_train, inoutput[\"outputlayer_output\"])\n",
    "\n",
    "#         predicts += np.argmax(inoutput[\"outputlayer_output\"], axis=0).tolist()\n",
    "#         golds += np.argmax(Y_train, axis=0).tolist()\n",
    "        \n",
    "#         inoutput = forward(X_test, parameters)\n",
    "#         #print(Y.shape)\n",
    "        \n",
    "#         #print(inoutput[\"outputlayer_output\"].shape)\n",
    "#         test_loss = cross_entropy(Y_test, inoutput[\"outputlayer_output\"])\n",
    "\n",
    "#         predicts_test += np.argmax(inoutput[\"outputlayer_output\"], axis=0).tolist()\n",
    "#         golds_test += np.argmax(Y_test, axis=0).tolist()\n",
    "        \n",
    "#         print(\"Epoch {}: training loss = {},  test loss = {}, Train_accur = {},Test_accur = {}\".format(\n",
    "#             i + 1, train_loss, test_loss, evaluation(predicts, golds), evaluation(predicts_test, golds_test)))\n",
    "\n",
    "#         TrainError.append(1 - evaluation(predicts, golds))\n",
    "#         TestError.append(1 - evaluation(predicts_test, golds_test))\n",
    "        \n",
    "        \n",
    "# #         new_x_axis = np.arange(0,500, 5)\n",
    "# #         fig, ax = plt.subplots(1, 1)\n",
    "# #         print(TrainError.shape)\n",
    "# #         print(new_x_axis.shape)\n",
    "# #         ax.plot(new_x_axis, TrainError)\n",
    "              \n",
    "        \n",
    "# #     with open(\"Train_error_rate.json\", mode=\"w\") as stream:\n",
    "# #         json.dump(TrainError, stream)\n",
    "\n",
    "# #     with open(\"Test_error_rate.json\", mode=\"w\") as stream:\n",
    "# #         json.dump(TestError, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
