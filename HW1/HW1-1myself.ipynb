{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = 10\n",
    "\n",
    "test = np.load('test.npz')\n",
    "train = np.load('train.npz')\n",
    "\n",
    "y_test = test['label']\n",
    "x_test = test['image']\n",
    "y_train = train['label']\n",
    "x_train = train['image']\n",
    "\n",
    "x_train = x_train.reshape(-1, x_train.shape[1] * x_train.shape[2])\n",
    "x_test = x_test.reshape(-1, x_test.shape[1] * x_test.shape[2])\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "X = np.vstack((x_train, x_test))\n",
    "y = np.vstack((y_train, y_test))\n",
    "\n",
    "examples = y.shape[0]\n",
    "y = y.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "  #print(y)\n",
    "\n",
    "m = x_train.shape[0]\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:, :m], Y_new[:, m:]\n",
    "\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "#     # To prevent from overflow\n",
    "#     z = np.clip(z, 1e-15, 1 - 1e-15)\n",
    "    s = sigmoid(z) * (1 - sigmoid(z))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y, Y_hat):\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(Y, Y_hat):\n",
    "    L = Y_hat - Y\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    s = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predicts, golds):\n",
    "    correct = 0\n",
    "    total = len(predicts)\n",
    "    assert len(predicts) == len(golds)\n",
    "    for predict, gold in zip(predicts, golds):\n",
    "        if predict == gold:\n",
    "            correct += 1\n",
    "    accurancy = correct / total\n",
    "    return accurancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Layer:\n",
    "#     def __init__(self, input_, output):\n",
    "#         self.input = input_\n",
    "#         self.output = output  # number of layer node\n",
    "#         self.W = np.random.randn(self.output, self.input) * np.sqrt(1. / self.input)\n",
    "#         self.b = np.zeros((self.output, 1)) * np.sqrt(1. / self.input)\n",
    "        \n",
    "#     def forward(self, last_layer):\n",
    "#         self.last_layer = last_layer\n",
    "#         layer_output_temp = np.matmul(self.W, self.last_layer) + self.b\n",
    "#         layer_output = sigmoid(layer_output_temp)\n",
    "#         return layer_output\n",
    "    \n",
    "#     def back_propagation(self, CE_gradientorgradient, m_batch, learning_rate):\n",
    "#         W_temp = self.W\n",
    "#         W_gradient = (1. / m_batch) * np.matmul(CE_gradientorgradient, self.last_layer.T)\n",
    "#         b_gradient = (1. / m_batch) * np.sum(CE_gradientorgradient, axis=1, keepdims=True)\n",
    "#         self.W_new = self.W - learning_rate * W_gradient\n",
    "#         self.b_new = self.b - learning_rate * b_gradient\n",
    "#         self.W = self.W_new\n",
    "#         self.b = self.b_new\n",
    "#         gradient_temp = np.matmul(W_temp.T, CE_gradientorgradient)\n",
    "#         return gradient_temp\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiddenlayer1 = Layer(784, 400)\n",
    "# hiddenlayer2 = Layer(400, 400)\n",
    "# outputlayer = Layer(400, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SGD_train_epoch(X_train, Y_train, batch_size = 64, epoch = 10, learning_rate = 0.03):\n",
    "#     for i in range(epoch):\n",
    "       \n",
    "#         # shuffle training set\n",
    "#         permutation = np.random.permutation(X_train.shape[1])\n",
    "#         X_train_shuffled = X_train[:, permutation]\n",
    "#         Y_train_shuffled = Y_train[:, permutation]\n",
    "    \n",
    "#         batch_num = len(X_train) // batch_size\n",
    "#         predicts = []\n",
    "#         golds = []\n",
    "#         predicts_test = []\n",
    "#         golds_test = []\n",
    "        \n",
    "#         for j in range(batch_num):\n",
    "#             begin = j * batch_size\n",
    "#             end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "#             X = X_train_shuffled[:, begin:end]\n",
    "#             Y = Y_train_shuffled[:, begin:end]\n",
    "#             m_batch = end - begin\n",
    "            \n",
    "#             output1_temp = hiddenlayer1.forward(X)\n",
    "#             output1 = sigmoid(output1_temp)\n",
    "#             output2_temp = hiddenlayer2.forward(output1)\n",
    "#             output2 = sigmoid(output2_temp)\n",
    "#             y_hat_temp = outputlayer.forward(output2)\n",
    "#             y_hat = softmax(y_hat_temp)\n",
    "#             #print(y_hat)\n",
    "            \n",
    "#             predicts += np.argmax(y_hat, axis = 0).tolist()\n",
    "#             golds += np.argmax(Y, axis = 0).tolist()\n",
    "            \n",
    "#             loss = cross_entropy(Y, y_hat)\n",
    "#             gradient = cross_entropy_gradient(Y, y_hat)\n",
    "            \n",
    "#             back_output1 = outputlayer.back_propagation(gradient, m_batch, learning_rate)\n",
    "#             back_output2_temp = sigmoid_gradient(output2_temp) * back_output1\n",
    "#             back_output2 = hiddenlayer2.back_propagation(back_output2_temp, m_batch, learning_rate)\n",
    "#             back_output3_temp = sigmoid_gradient(output1_temp) * back_output2\n",
    "#             back_output3 = hiddenlayer1.back_propagation(back_output3_temp, m_batch, learning_rate)\n",
    "        \n",
    "#         print('Epoch : ', i + 1, 'training_loss = ', loss, 'train_accur = ', evaluation(predicts, golds))\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD_train_epoch(X_train, Y_train, batch_size = 64, epoch = 1000, learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"W1\": np.random.randn(400, 784) * np.sqrt(1. / 784),\n",
    "              \"b1\": np.zeros((400, 1)) * np.sqrt(1. / 784),\n",
    "              \"W2\": np.random.randn(400, 400) * np.sqrt(1. / 400),\n",
    "              \"b2\": np.zeros((400, 1)) * np.sqrt(1. / 400),\n",
    "              \"W3\": np.random.randn(digits, 400) * np.sqrt(1. / 400),\n",
    "              \"b3\": np.zeros((digits, 1)) * np.sqrt(1. / 400)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, parameters):\n",
    "    inoutput = {}\n",
    "    inoutput[\"hiddenlayer1_output_temp\"] = np.matmul(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
    "    inoutput[\"hiddenlayer1_output\"] = sigmoid(inoutput[\"hiddenlayer1_output_temp\"])\n",
    "    \n",
    "    inoutput[\"hiddenlayer2_output_temp\"] = np.matmul(parameters[\"W2\"], inoutput[\"hiddenlayer1_output\"]) + parameters[\"b2\"]\n",
    "    inoutput[\"hiddenlayer2_output\"] = sigmoid(inoutput[\"hiddenlayer2_output_temp\"])\n",
    "    \n",
    "    inoutput[\"outputlayer_output_temp\"] = np.matmul(parameters[\"W3\"], inoutput[\"hiddenlayer2_output\"]) + parameters[\"b3\"]\n",
    "    inoutput[\"outputlayer_output\"] = softmax(inoutput[\"outputlayer_output_temp\"])\n",
    "    #inoutput[\"outputlayer_output\"] = y_hat\n",
    "\n",
    "    return inoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X, Y, parameters, inoutput, m_batch):\n",
    "    CE_gradient = inoutput[\"outputlayer_output\"] - Y\n",
    "\n",
    "    W3_gradient = (1. / m_batch) * np.matmul(CE_gradient, inoutput[\"hiddenlayer2_output\"].T)\n",
    "    b3_gradient = (1. / m_batch) * np.sum(CE_gradient, axis=1, keepdims=True)\n",
    "\n",
    "    # ---\n",
    "\n",
    "    outputlayer_backward_output = np.matmul(parameters[\"W3\"].T, CE_gradient)\n",
    "    hiddenlayer2_backward_input = outputlayer_backward_output * sigmoid_gradient(inoutput[\"hiddenlayer2_output_temp\"])\n",
    "\n",
    "    W2_gradient = (1. / m_batch) * np.matmul(hiddenlayer2_backward_input, inoutput[\"hiddenlayer1_output\"].T)\n",
    "    b2_gradient = (1. / m_batch) * np.sum(hiddenlayer2_backward_input, axis=1, keepdims=True)\n",
    "\n",
    "    # ---\n",
    "\n",
    "    hiddenlayer2_backward_output = np.matmul(parameters[\"W2\"].T, hiddenlayer2_backward_input)\n",
    "    hiddenlayer1_backward_input = hiddenlayer2_backward_output * sigmoid_gradient(inoutput[\"hiddenlayer1_output_temp\"])\n",
    "\n",
    "    W1_gradient = (1. / m_batch) * np.matmul(hiddenlayer1_backward_input, X.T)\n",
    "    b1_gradient = (1. / m_batch) * np.sum(hiddenlayer1_backward_input, axis=1, keepdims=True)\n",
    "\n",
    "    Wb_gradients = {\"W1_gradient\": W1_gradient, \"b1_gradient\": b1_gradient, \"W2_gradient\": W2_gradient,\n",
    "                     \"b2_gradient\": b2_gradient, \"W3_gradient\": W3_gradient, \"b3_gradient\": b3_gradient}\n",
    "\n",
    "    return Wb_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bunlab/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 2.19194169068331,  test loss = 2.1997713139164023, Train_accur = 0.28725,Test_accur = 0.27063106796116504\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9fda0acc86f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mnew_x_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epoch = 3\n",
    "    batch_size = 64\n",
    "    TrainError = []\n",
    "    TestError = []\n",
    "    for i in range(epoch):\n",
    "       \n",
    "        # shuffle training set\n",
    "        permutation = np.random.permutation(X_train.shape[1])\n",
    "        X_train_shuffled = X_train[:, permutation]\n",
    "        Y_train_shuffled = Y_train[:, permutation]\n",
    "    \n",
    "        batch_num = len(X_train) // batch_size\n",
    "        predicts = []\n",
    "        golds = []\n",
    "        predicts_test = []\n",
    "        golds_test = []\n",
    "        learning_rate = 0.03\n",
    "        \n",
    "        \n",
    "        for j in range(batch_num):\n",
    "            begin = j * batch_size\n",
    "            end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "            X = X_train_shuffled[:, begin:end]\n",
    "            Y = Y_train_shuffled[:, begin:end]\n",
    "            m_batch = end - begin\n",
    "            \n",
    "            inoutput = forward(X, parameters)\n",
    "            Wb_gradients = back_propagation(X, Y, parameters, inoutput, m_batch)\n",
    "            \n",
    "            W1_gradient = Wb_gradients[\"W1_gradient\"]\n",
    "            b1_gradient = Wb_gradients[\"b1_gradient\"]\n",
    "            W2_gradient = Wb_gradients[\"W2_gradient\"]\n",
    "            b2_gradient = Wb_gradients[\"b2_gradient\"]\n",
    "            W3_gradient = Wb_gradients[\"W3_gradient\"]\n",
    "            b3_gradient = Wb_gradients[\"b3_gradient\"]\n",
    "            \n",
    "            parameters[\"W1\"] = parameters[\"W1\"] - learning_rate * W1_gradient\n",
    "            parameters[\"b1\"] = parameters[\"b1\"] - learning_rate * b1_gradient\n",
    "            parameters[\"W2\"] = parameters[\"W2\"] - learning_rate * W2_gradient\n",
    "            parameters[\"b2\"] = parameters[\"b2\"] - learning_rate * b2_gradient\n",
    "            parameters[\"W3\"] = parameters[\"W3\"] - learning_rate * W3_gradient\n",
    "            parameters[\"b3\"] = parameters[\"b3\"] - learning_rate * b3_gradient\n",
    "            \n",
    "            \n",
    "          \n",
    "        inoutput = forward(X_train, parameters)\n",
    "        #print(Y.shape)\n",
    "        \n",
    "        #print(inoutput[\"outputlayer_output\"].shape)\n",
    "        train_loss = cross_entropy(Y_train, inoutput[\"outputlayer_output\"])\n",
    "\n",
    "        predicts += np.argmax(inoutput[\"outputlayer_output\"], axis=0).tolist()\n",
    "        golds += np.argmax(Y_train, axis=0).tolist()\n",
    "        \n",
    "        inoutput = forward(X_test, parameters)\n",
    "        #print(Y.shape)\n",
    "        \n",
    "        #print(inoutput[\"outputlayer_output\"].shape)\n",
    "        test_loss = cross_entropy(Y_test, inoutput[\"outputlayer_output\"])\n",
    "\n",
    "        predicts_test += np.argmax(inoutput[\"outputlayer_output\"], axis=0).tolist()\n",
    "        golds_test += np.argmax(Y_test, axis=0).tolist()\n",
    "        \n",
    "        print(\"Epoch {}: training loss = {},  test loss = {}, Train_accur = {},Test_accur = {}\".format(\n",
    "            i + 1, train_loss, test_loss, evaluation(predicts, golds), evaluation(predicts_test, golds_test)))\n",
    "\n",
    "        TrainError.append(1 - evaluation(predicts, golds))\n",
    "        TestError.append(1 - evaluation(predicts_test, golds_test))\n",
    "        \n",
    "        \n",
    "        new_x_axis = np.arange(0,500, 5)\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        print(TrainError.shape)\n",
    "        print(new_x_axis.shape)\n",
    "        ax.plot(new_x_axis, TrainError)\n",
    "              \n",
    "        \n",
    "#     with open(\"Train_error_rate.json\", mode=\"w\") as stream:\n",
    "#         json.dump(TrainError, stream)\n",
    "\n",
    "#     with open(\"Test_error_rate.json\", mode=\"w\") as stream:\n",
    "#         json.dump(TestError, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
