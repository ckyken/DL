{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('test.npz')\n",
    "train = np.load('train.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label']\n",
    "np.unique(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = test['label']\n",
    "\n",
    "np.zeros((y.size, 10)).shape\n",
    "np.arange(y.size)\n",
    "\n",
    "new_y = np.eye(10)[[int(y_) for y_ in y.tolist()]]\n",
    "new_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 28, 28)\n",
      "(12000, 784)\n",
      "(784, 12000)\n"
     ]
    }
   ],
   "source": [
    "y_test = test['label']\n",
    "X_test = test['image']\n",
    "y_train = train['label']\n",
    "X_train = train['image']\n",
    "\n",
    "# transfer to one-hot vector\n",
    "#y_onehot_train = np.zeros((y_train.size, 10))\n",
    "#y_onehot_train[np.arange(y_train.size), y_train] = 1\n",
    "#y_onehot_test = np.zeros((y_test.size, 10))\n",
    "#y_onehot_test[np.arange(y_test.size), y_train] = 1\n",
    "y_onehot_train = np.eye(10)[[int(y_) for y_ in y_train.tolist()]]\n",
    "y_onehot_test = np.eye(10)[[int(y_) for y_ in y_test.tolist()]]\n",
    "\n",
    "\n",
    "y_train = np.moveaxis(y_onehot_train, 1, 0)\n",
    "print(X_train.shape)\n",
    "X_train = X_train.reshape(-1, X_train.shape[1] * X_train.shape[2])\n",
    "print(X_train.shape)\n",
    "X_train = np.moveaxis(X_train, 1, 0)\n",
    "print(X_train.shape)\n",
    "\n",
    "y_test = np.moveaxis(y_onehot_test, 1, 0)\n",
    "X_test = X_test.reshape(-1, X_test.shape[1] * X_test.shape[2])\n",
    "X_test = np.moveaxis(X_test, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train < 100] = -1\n",
    "X_train[X_train >= 100] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, last_layer_num, node_num, activation_function=None, custom_W=None, custom_b=None):\n",
    "        self.__node_num = node_num  #output_dim\n",
    "        self.__last_layer_num = last_layer_num  #input_dim\n",
    "\n",
    "        \n",
    "        self.__layer_input = None\n",
    "        self.activation = activation_function\n",
    "        \n",
    "        if custom_W is not None:\n",
    "            self.W_ = custom_W\n",
    "        else:\n",
    "            limit = 1 / np.sqrt(self.__last_layer_num)\n",
    "            self.W_ = np.random.uniform(-limit, limit, (self.__last_layer_num, self.__node_num))\n",
    "        \n",
    "        if custom_b is not None:\n",
    "            self.b_ = custom_b\n",
    "        else:\n",
    "            self.b_ = np.zeros((self.__node_num, 1))\n",
    "\n",
    "    def forward_propagation(self, last_layer):\n",
    "        self.__layer_input = last_layer\n",
    "        if self.activation:\n",
    "            return np.dot(self.W_.T, last_layer) + self.b_\n",
    "        else:\n",
    "            return np.dot(self.W_.T, last_layer) + self.b_\n",
    "        \n",
    "    \n",
    "    def back_propagation(self, gradient, learning_rate = 0.0001):\n",
    "        if self.activation:\n",
    "            gradient = self.activation.gradient(gradient)\n",
    "        \n",
    "        W_temp = self.W_\n",
    "        \n",
    "        gradient_W = self.__layer_input.dot(gradient.T)\n",
    "        gradient_b = np.sum(gradient, axis=1, keepdims=True)\n",
    "        \n",
    "        self.W_ = self.W_ - learning_rate * gradient_W\n",
    "        assert self.W_.shape == gradient_W.shape\n",
    "        self.b_ = self.b_ - learning_rate * gradient_b\n",
    "        assert self.b_.shape == gradient_b.shape\n",
    "        \n",
    "        accumulated_gradient = W_temp.dot(gradient)\n",
    "        return accumulated_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26 27]\n",
      " [26 27]]\n",
      "[[ 0.61079314 -0.00506093  0.28955549]\n",
      " [-0.19369355  0.15849927 -0.18915272]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10.84458942, 11.26168901],\n",
       "       [ 3.98939685,  4.14283519],\n",
       "       [ 2.61047192,  2.71087469]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test forward propagation\n",
    "\n",
    "W = np.array([[1, 2, 3], [1, 2, 3]]).T\n",
    "b = np.array([0, 1])\n",
    "data = np.array([[3, 4, 5],[3, 4, 5]]).T\n",
    "\n",
    "layer = Layer(3, 2, None, W, b)\n",
    "layer1_output = layer.forward_propagation(data)\n",
    "\n",
    "print(layer1_output)\n",
    "\n",
    "layer2 = Layer(2, 3)\n",
    "print(layer2.W_)\n",
    "layer2.forward_propagation(layer1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # 減max會比較穩定,避免overflow\n",
    "    return e_x / e_x.sum(axis=0)  # -xmax會消掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __call__(self,x):\n",
    "        # To prevent from overflow\n",
    "        x = np.clip(x, 1e-15, 1 - 1e-15)\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    def gradient(self, x):\n",
    "        return self(x) * (1.0 - self(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, x):\n",
    "        return np.maximum(x, 0.0)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        x[x <= 0] = 0.0\n",
    "        x[x > 0] = 1.0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy():\n",
    "    def __call__(self, y_hat, y):\n",
    "        # Avoid division by zero\n",
    "        y_hat = np.clip(y_hat, 1e-15, 1 - 1e-15)\n",
    "        # return\n",
    "        loss_sum = 0\n",
    "        for n in range(y.shape[0]):\n",
    "            for k in range(y.shape[1]):\n",
    "                if y[n, k] == 1:\n",
    "                    loss_sum += -np.log(y_hat[n, k])\n",
    "        loss = np.sum(loss_sum) / y.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def gradient(self, y_hat, y):\n",
    "        return y_hat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy2():\n",
    "    def __call__(self, y_hat, y):\n",
    "        # Avoid division by zero\n",
    "        y_hat = np.clip(y_hat, 1e-15, 1 - 1e-15)\n",
    "        # softmax\n",
    "        m = y.shape[0]\n",
    "        p = Softmax(y_hat)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        \n",
    "        log_likelihood = -np.log(p[range(m), y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, y_hat, y):\n",
    "        return y_hat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.992007221626415e-16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # test \n",
    "y = np.array([[1, 0 , 0], [1, 0, 0], [0 , 1, 0]])\n",
    "y_hat = np.array([[3, 2, 2], [2, 5 ,6], [4 ,2 ,6]])\n",
    "\n",
    "CrossEntropy()(y_hat, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiddenLayer = Layer(last_layer_num=784, node_num=392, activation_function=Sigmoid())\n",
    "HiddenLayer2 = Layer(last_layer_num=392, node_num=198, activation_function=Sigmoid())\n",
    "HiddenLayer3 = Layer(last_layer_num=198, node_num=2, activation_function=Sigmoid())\n",
    "OutputLayer = Layer(last_layer_num=2, node_num=10)\n",
    "#loss_func = CrossEntropy()\n",
    "loss_func = CrossEntropy2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X_train, y_train, epoch=10, learning_rate=1e-4):\n",
    "    \"\"\" Gradient Descent \"\"\"\n",
    "    for i in range(epoch):\n",
    "        output1 = HiddenLayer.forward_propagation(X_train)\n",
    "        theta = OutputLayer.forward_propagation(output1)\n",
    "        y_hat = Softmax(theta)\n",
    "\n",
    "        loss = loss_func(y_hat, y_train)\n",
    "\n",
    "        print(loss)\n",
    "        #print(y_hat)\n",
    "        \n",
    "        gradient = loss_func.gradient(y_hat, y_train)\n",
    "\n",
    "        gradient = OutputLayer.back_propagation(gradient, learning_rate=learning_rate)\n",
    "        HiddenLayer.back_propagation(gradient, learning_rate=learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_epoch(X_train, y_train, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_train_epoch(X_train, y_train, batch_size=16, epoch=10, learning_rate=1e-10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    batch_num = 0\n",
    "    all_loss = []\n",
    "    for i in range(epoch):\n",
    "        epoch_loss = []\n",
    "        for start in range(0, X_train.shape[1], batch_size):\n",
    "            end = min(X_train.shape[1], start + batch_size)\n",
    "            \n",
    "            X = X_train[:, start:end]\n",
    "            y = y_train[:, start:end]\n",
    "            \n",
    "            #import ipdb; ipdb.set_trace()\n",
    "            \n",
    "            output1 = HiddenLayer.forward_propagation(X)\n",
    "            output2 = HiddenLayer2.forward_propagation(output1)\n",
    "            output3 = HiddenLayer3.forward_propagation(output2)\n",
    "            theta = OutputLayer.forward_propagation(output3)\n",
    "            \n",
    "            y_hat = Softmax(theta)\n",
    "\n",
    "            #loss = loss_func(y_hat, y) / (end - start)\n",
    "            loss = loss_func(theta, y) / (end - start)\n",
    "            epoch_loss.append(loss)\n",
    "            \n",
    "            \n",
    "            if loss > 100:\n",
    "                import ipdb; ipdb.set_trace()\n",
    "            #print(y_hat)\n",
    "\n",
    "            gradient = loss_func.gradient(y_hat, y) / (end - start)\n",
    "\n",
    "            gradient = OutputLayer.back_propagation(gradient, learning_rate=learning_rate)\n",
    "            gradient = HiddenLayer3.back_propagation(gradient, learning_rate=learning_rate)\n",
    "            gradient = HiddenLayer2.back_propagation(gradient, learning_rate=learning_rate)\n",
    "            HiddenLayer.back_propagation(gradient, learning_rate=learning_rate)\n",
    "\n",
    "        print('Epoch:', i+1, 'Average loss:', sum(epoch_loss)/len(epoch_loss))\n",
    "            \n",
    "        all_loss.extend(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Average loss: 0.14889816700595176\n",
      "Epoch: 2 Average loss: 0.14889910322543742\n",
      "Epoch: 3 Average loss: 0.148900039346592\n",
      "Epoch: 4 Average loss: 0.1489009753966413\n",
      "Epoch: 5 Average loss: 0.14890191050480797\n",
      "Epoch: 6 Average loss: 0.1489028456415601\n",
      "Epoch: 7 Average loss: 0.14890378087553985\n",
      "Epoch: 8 Average loss: 0.14890471561578866\n",
      "Epoch: 9 Average loss: 0.14890565017313745\n",
      "Epoch: 10 Average loss: 0.1489065842285539\n",
      "Epoch: 11 Average loss: 0.14890751778158445\n",
      "Epoch: 12 Average loss: 0.1489084513269892\n",
      "Epoch: 13 Average loss: 0.14890938457596853\n",
      "Epoch: 14 Average loss: 0.1489103173876182\n",
      "Epoch: 15 Average loss: 0.14891124987268123\n",
      "Epoch: 16 Average loss: 0.14891218241182705\n",
      "Epoch: 17 Average loss: 0.14891311467349497\n",
      "Epoch: 18 Average loss: 0.148914046380684\n",
      "Epoch: 19 Average loss: 0.14891497767925888\n",
      "Epoch: 20 Average loss: 0.14891590892164583\n",
      "Epoch: 21 Average loss: 0.14891683994861205\n",
      "Epoch: 22 Average loss: 0.14891777090556663\n",
      "Epoch: 23 Average loss: 0.14891870172725022\n",
      "Epoch: 24 Average loss: 0.14891963227473023\n",
      "Epoch: 25 Average loss: 0.14892056230982642\n",
      "Epoch: 26 Average loss: 0.14892149204090274\n",
      "Epoch: 27 Average loss: 0.14892242182579388\n",
      "Epoch: 28 Average loss: 0.14892335154632375\n",
      "Epoch: 29 Average loss: 0.14892428126157659\n",
      "Epoch: 30 Average loss: 0.14892521064498843\n",
      "Epoch: 31 Average loss: 0.14892613976214172\n",
      "Epoch: 32 Average loss: 0.148927068911454\n",
      "Epoch: 33 Average loss: 0.14892799835473927\n",
      "Epoch: 34 Average loss: 0.14892892775936561\n",
      "Epoch: 35 Average loss: 0.14892985700569605\n",
      "Epoch: 36 Average loss: 0.14893078635261284\n",
      "Epoch: 37 Average loss: 0.14893171596578536\n",
      "Epoch: 38 Average loss: 0.14893264469167694\n",
      "Epoch: 39 Average loss: 0.14893357319347764\n",
      "Epoch: 40 Average loss: 0.14893450179622839\n",
      "Epoch: 41 Average loss: 0.1489354304207251\n",
      "Epoch: 42 Average loss: 0.14893635871836064\n",
      "Epoch: 43 Average loss: 0.14893728669450412\n",
      "Epoch: 44 Average loss: 0.1489382144349556\n",
      "Epoch: 45 Average loss: 0.14893914205413974\n",
      "Epoch: 46 Average loss: 0.14894006954046782\n",
      "Epoch: 47 Average loss: 0.14894099690783336\n",
      "Epoch: 48 Average loss: 0.14894192398875652\n",
      "Epoch: 49 Average loss: 0.14894285089492296\n",
      "Epoch: 50 Average loss: 0.14894377771139541\n",
      "Epoch: 51 Average loss: 0.1489447046359481\n",
      "Epoch: 52 Average loss: 0.14894563071585495\n",
      "Epoch: 53 Average loss: 0.14894655650831654\n",
      "Epoch: 54 Average loss: 0.14894748214808443\n",
      "Epoch: 55 Average loss: 0.14894840775966384\n",
      "Epoch: 56 Average loss: 0.14894933346594402\n",
      "Epoch: 57 Average loss: 0.14895025893565778\n",
      "Epoch: 58 Average loss: 0.14895118406980015\n",
      "Epoch: 59 Average loss: 0.14895210897923514\n",
      "Epoch: 60 Average loss: 0.14895303369298668\n",
      "Epoch: 61 Average loss: 0.14895395818610388\n",
      "Epoch: 62 Average loss: 0.148954882600725\n",
      "Epoch: 63 Average loss: 0.14895580711944736\n",
      "Epoch: 64 Average loss: 0.14895673140827226\n",
      "Epoch: 65 Average loss: 0.14895765608700118\n",
      "Epoch: 66 Average loss: 0.14895858053034147\n",
      "Epoch: 67 Average loss: 0.1489595048183278\n",
      "Epoch: 68 Average loss: 0.14896042979122517\n",
      "Epoch: 69 Average loss: 0.1489613547035537\n",
      "Epoch: 70 Average loss: 0.14896227927914835\n",
      "Epoch: 71 Average loss: 0.1489632035546816\n",
      "Epoch: 72 Average loss: 0.148964127770184\n",
      "Epoch: 73 Average loss: 0.14896505196181573\n",
      "Epoch: 74 Average loss: 0.14896597694359442\n",
      "Epoch: 75 Average loss: 0.14896690211953748\n",
      "Epoch: 76 Average loss: 0.14896782704339476\n",
      "Epoch: 77 Average loss: 0.1489687518258627\n",
      "Epoch: 78 Average loss: 0.14896967678047546\n",
      "Epoch: 79 Average loss: 0.14897060182957925\n",
      "Epoch: 80 Average loss: 0.14897152665094318\n",
      "Epoch: 81 Average loss: 0.1489724511088225\n",
      "Epoch: 82 Average loss: 0.14897337543879974\n",
      "Epoch: 83 Average loss: 0.1489742996611621\n",
      "Epoch: 84 Average loss: 0.148975223658764\n",
      "Epoch: 85 Average loss: 0.14897614685396177\n",
      "Epoch: 86 Average loss: 0.1489770699330818\n",
      "Epoch: 87 Average loss: 0.14897799294380876\n",
      "Epoch: 88 Average loss: 0.14897891593181348\n",
      "Epoch: 89 Average loss: 0.14897983930238853\n",
      "Epoch: 90 Average loss: 0.1489807624623919\n",
      "Epoch: 91 Average loss: 0.14898168535167877\n",
      "Epoch: 92 Average loss: 0.1489826082164871\n",
      "Epoch: 93 Average loss: 0.14898353105181392\n",
      "Epoch: 94 Average loss: 0.14898445353596657\n",
      "Epoch: 95 Average loss: 0.14898537580588586\n",
      "Epoch: 96 Average loss: 0.1489862981420543\n",
      "Epoch: 97 Average loss: 0.14898722029783654\n",
      "Epoch: 98 Average loss: 0.1489881420918321\n",
      "Epoch: 99 Average loss: 0.14898906396088848\n",
      "Epoch: 100 Average loss: 0.1489899853421513\n",
      "Epoch: 101 Average loss: 0.14899090563039882\n",
      "Epoch: 102 Average loss: 0.1489918256705148\n",
      "Epoch: 103 Average loss: 0.14899274570840967\n",
      "Epoch: 104 Average loss: 0.14899366532319333\n",
      "Epoch: 105 Average loss: 0.14899458337141464\n",
      "Epoch: 106 Average loss: 0.14899550096312134\n",
      "Epoch: 107 Average loss: 0.148996417642629\n",
      "Epoch: 108 Average loss: 0.14899733424386055\n",
      "Epoch: 109 Average loss: 0.14899825036253464\n",
      "Epoch: 110 Average loss: 0.1489991663549446\n",
      "Epoch: 111 Average loss: 0.14900008191524697\n",
      "Epoch: 112 Average loss: 0.14900099692683588\n",
      "Epoch: 113 Average loss: 0.14900191182301759\n",
      "Epoch: 114 Average loss: 0.14900282646139504\n",
      "Epoch: 115 Average loss: 0.149003740963989\n",
      "Epoch: 116 Average loss: 0.14900465538438018\n",
      "Epoch: 117 Average loss: 0.14900557008796048\n",
      "Epoch: 118 Average loss: 0.14900648535858\n",
      "Epoch: 119 Average loss: 0.14900740021616724\n",
      "Epoch: 120 Average loss: 0.14900831476100496\n",
      "Epoch: 121 Average loss: 0.14900922929535362\n",
      "Epoch: 122 Average loss: 0.14901014350731243\n",
      "Epoch: 123 Average loss: 0.14901105736922068\n",
      "Epoch: 124 Average loss: 0.14901197115394452\n",
      "Epoch: 125 Average loss: 0.14901288441648775\n",
      "Epoch: 126 Average loss: 0.149013796977012\n",
      "Epoch: 127 Average loss: 0.14901470910114\n",
      "Epoch: 128 Average loss: 0.14901562122783488\n",
      "Epoch: 129 Average loss: 0.14901653342631108\n",
      "Epoch: 130 Average loss: 0.14901744539149342\n",
      "Epoch: 131 Average loss: 0.14901835719033493\n",
      "Epoch: 132 Average loss: 0.14901926885881187\n",
      "Epoch: 133 Average loss: 0.1490201804450954\n",
      "Epoch: 134 Average loss: 0.1490210920474989\n",
      "Epoch: 135 Average loss: 0.1490220033094547\n",
      "Epoch: 136 Average loss: 0.14902291457712744\n",
      "Epoch: 137 Average loss: 0.14902382595870575\n",
      "Epoch: 138 Average loss: 0.1490247374286051\n",
      "Epoch: 139 Average loss: 0.14902564872417892\n",
      "Epoch: 140 Average loss: 0.1490265596614633\n",
      "Epoch: 141 Average loss: 0.14902746986395077\n",
      "Epoch: 142 Average loss: 0.1490283792676486\n",
      "Epoch: 143 Average loss: 0.14902928838530527\n",
      "Epoch: 144 Average loss: 0.149030197427442\n",
      "Epoch: 145 Average loss: 0.14903110658469726\n",
      "Epoch: 146 Average loss: 0.14903201525485477\n",
      "Epoch: 147 Average loss: 0.14903292335136412\n",
      "Epoch: 148 Average loss: 0.14903383133799947\n",
      "Epoch: 149 Average loss: 0.1490347389840156\n",
      "Epoch: 150 Average loss: 0.14903564643609404\n",
      "Epoch: 151 Average loss: 0.14903655390846907\n",
      "Epoch: 152 Average loss: 0.14903746149700128\n",
      "Epoch: 153 Average loss: 0.1490383684283618\n",
      "Epoch: 154 Average loss: 0.14903927503812378\n",
      "Epoch: 155 Average loss: 0.14904018041489756\n",
      "Epoch: 156 Average loss: 0.14904108564441856\n",
      "Epoch: 157 Average loss: 0.14904199049404407\n",
      "Epoch: 158 Average loss: 0.14904289496817352\n",
      "Epoch: 159 Average loss: 0.1490437994029976\n",
      "Epoch: 160 Average loss: 0.14904470395528618\n",
      "Epoch: 161 Average loss: 0.1490456081937018\n",
      "Epoch: 162 Average loss: 0.14904651216815457\n",
      "Epoch: 163 Average loss: 0.14904741614767567\n",
      "Epoch: 164 Average loss: 0.1490483200669857\n",
      "Epoch: 165 Average loss: 0.1490492238453311\n",
      "Epoch: 166 Average loss: 0.149050127650148\n",
      "Epoch: 167 Average loss: 0.14905103130330555\n",
      "Epoch: 168 Average loss: 0.14905193464038466\n",
      "Epoch: 169 Average loss: 0.14905283788851215\n",
      "Epoch: 170 Average loss: 0.14905374096292612\n",
      "Epoch: 171 Average loss: 0.1490546436403061\n",
      "Epoch: 172 Average loss: 0.14905554609394026\n",
      "Epoch: 173 Average loss: 0.1490564474253262\n",
      "Epoch: 174 Average loss: 0.1490573480451776\n",
      "Epoch: 175 Average loss: 0.1490582484696804\n",
      "Epoch: 176 Average loss: 0.1490591488433221\n",
      "Epoch: 177 Average loss: 0.1490600491308866\n",
      "Epoch: 178 Average loss: 0.1490609489032845\n",
      "Epoch: 179 Average loss: 0.14906184853078192\n",
      "Epoch: 180 Average loss: 0.149062747218271\n",
      "Epoch: 181 Average loss: 0.14906364581114473\n",
      "Epoch: 182 Average loss: 0.1490645445218145\n",
      "Epoch: 183 Average loss: 0.1490654431409263\n",
      "Epoch: 184 Average loss: 0.14906634175365888\n",
      "Epoch: 185 Average loss: 0.14906724010511568\n",
      "Epoch: 186 Average loss: 0.1490681383492506\n",
      "Epoch: 187 Average loss: 0.14906903644591793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 188 Average loss: 0.14906993377220437\n",
      "Epoch: 189 Average loss: 0.14907083030932466\n",
      "Epoch: 190 Average loss: 0.1490717268121669\n",
      "Epoch: 191 Average loss: 0.14907262309116595\n",
      "Epoch: 192 Average loss: 0.14907351909919325\n",
      "Epoch: 193 Average loss: 0.14907441482220893\n",
      "Epoch: 194 Average loss: 0.14907531027548046\n",
      "Epoch: 195 Average loss: 0.14907620523260193\n",
      "Epoch: 196 Average loss: 0.14907709995401758\n",
      "Epoch: 197 Average loss: 0.14907799388143103\n",
      "Epoch: 198 Average loss: 0.1490788872463811\n",
      "Epoch: 199 Average loss: 0.14907978051459256\n",
      "Epoch: 200 Average loss: 0.1490806738310016\n",
      "Epoch: 201 Average loss: 0.1490815671692914\n",
      "Epoch: 202 Average loss: 0.14908246086739316\n",
      "Epoch: 203 Average loss: 0.14908335438842701\n",
      "Epoch: 204 Average loss: 0.1490842470607822\n",
      "Epoch: 205 Average loss: 0.1490851397629674\n",
      "Epoch: 206 Average loss: 0.1490860317488965\n",
      "Epoch: 207 Average loss: 0.14908692310005456\n",
      "Epoch: 208 Average loss: 0.14908781415108577\n",
      "Epoch: 209 Average loss: 0.1490887052037983\n",
      "Epoch: 210 Average loss: 0.14908959638263183\n",
      "Epoch: 211 Average loss: 0.14909048741847428\n",
      "Epoch: 212 Average loss: 0.14909137805648967\n",
      "Epoch: 213 Average loss: 0.14909226843683301\n",
      "Epoch: 214 Average loss: 0.14909315882541177\n",
      "Epoch: 215 Average loss: 0.14909404930394696\n",
      "Epoch: 216 Average loss: 0.14909493965588969\n",
      "Epoch: 217 Average loss: 0.14909582976902377\n",
      "Epoch: 218 Average loss: 0.1490967195625142\n",
      "Epoch: 219 Average loss: 0.1490976090388161\n",
      "Epoch: 220 Average loss: 0.14909849843343687\n",
      "Epoch: 221 Average loss: 0.14909938766640277\n",
      "Epoch: 222 Average loss: 0.14910027634041692\n",
      "Epoch: 223 Average loss: 0.14910116465630435\n",
      "Epoch: 224 Average loss: 0.1491020527457176\n",
      "Epoch: 225 Average loss: 0.1491029402778584\n",
      "Epoch: 226 Average loss: 0.1491038274819961\n",
      "Epoch: 227 Average loss: 0.1491047135281105\n",
      "Epoch: 228 Average loss: 0.1491055991948467\n",
      "Epoch: 229 Average loss: 0.14910648439306912\n",
      "Epoch: 230 Average loss: 0.14910736921554832\n",
      "Epoch: 231 Average loss: 0.14910825421523038\n",
      "Epoch: 232 Average loss: 0.14910913928541344\n",
      "Epoch: 233 Average loss: 0.1491100240679865\n",
      "Epoch: 234 Average loss: 0.14911090889593898\n",
      "Epoch: 235 Average loss: 0.1491117932009647\n",
      "Epoch: 236 Average loss: 0.14911267726043806\n",
      "Epoch: 237 Average loss: 0.1491135616403164\n",
      "Epoch: 238 Average loss: 0.14911444599171558\n",
      "Epoch: 239 Average loss: 0.14911532998748034\n",
      "Epoch: 240 Average loss: 0.14911621354228627\n",
      "Epoch: 241 Average loss: 0.14911709682378285\n",
      "Epoch: 242 Average loss: 0.1491179794136278\n",
      "Epoch: 243 Average loss: 0.14911886163998786\n",
      "Epoch: 244 Average loss: 0.14911974345000864\n",
      "Epoch: 245 Average loss: 0.14912062561208023\n",
      "Epoch: 246 Average loss: 0.14912150754388184\n",
      "Epoch: 247 Average loss: 0.14912238920769783\n",
      "Epoch: 248 Average loss: 0.14912327071503836\n",
      "Epoch: 249 Average loss: 0.14912415235495086\n",
      "Epoch: 250 Average loss: 0.14912503405279737\n",
      "Epoch: 251 Average loss: 0.1491259156206884\n",
      "Epoch: 252 Average loss: 0.1491267971723256\n",
      "Epoch: 253 Average loss: 0.14912767842219699\n",
      "Epoch: 254 Average loss: 0.1491285594580314\n",
      "Epoch: 255 Average loss: 0.1491294397710987\n",
      "Epoch: 256 Average loss: 0.14913031901677315\n",
      "Epoch: 257 Average loss: 0.14913119765011995\n",
      "Epoch: 258 Average loss: 0.14913207608361975\n",
      "Epoch: 259 Average loss: 0.14913295456272085\n",
      "Epoch: 260 Average loss: 0.14913383344477252\n",
      "Epoch: 261 Average loss: 0.14913471212682466\n",
      "Epoch: 262 Average loss: 0.14913559086891548\n",
      "Epoch: 263 Average loss: 0.14913646934846603\n",
      "Epoch: 264 Average loss: 0.1491373470348814\n",
      "Epoch: 265 Average loss: 0.14913822411122496\n",
      "Epoch: 266 Average loss: 0.14913910077577905\n",
      "Epoch: 267 Average loss: 0.14913997674600582\n",
      "Epoch: 268 Average loss: 0.1491408526280211\n",
      "Epoch: 269 Average loss: 0.14914172840908355\n",
      "Epoch: 270 Average loss: 0.14914260342285632\n",
      "Epoch: 271 Average loss: 0.1491434773417729\n",
      "Epoch: 272 Average loss: 0.1491443511766076\n",
      "Epoch: 273 Average loss: 0.1491452252872088\n",
      "Epoch: 274 Average loss: 0.14914609922368358\n",
      "Epoch: 275 Average loss: 0.14914697321744225\n",
      "Epoch: 276 Average loss: 0.1491478468136906\n",
      "Epoch: 277 Average loss: 0.14914872104025884\n",
      "Epoch: 278 Average loss: 0.14914959510069958\n",
      "Epoch: 279 Average loss: 0.14915046920181035\n",
      "Epoch: 280 Average loss: 0.14915134288232174\n",
      "Epoch: 281 Average loss: 0.14915221585414068\n",
      "Epoch: 282 Average loss: 0.14915308858412382\n",
      "Epoch: 283 Average loss: 0.14915396120334226\n",
      "Epoch: 284 Average loss: 0.14915483367673726\n",
      "Epoch: 285 Average loss: 0.14915570485137924\n",
      "Epoch: 286 Average loss: 0.14915657593654744\n",
      "Epoch: 287 Average loss: 0.14915744732151628\n",
      "Epoch: 288 Average loss: 0.1491583181139796\n",
      "Epoch: 289 Average loss: 0.14915918861546387\n",
      "Epoch: 290 Average loss: 0.14916005925702208\n",
      "Epoch: 291 Average loss: 0.14916092998849675\n",
      "Epoch: 292 Average loss: 0.14916180035087326\n",
      "Epoch: 293 Average loss: 0.1491626702994793\n",
      "Epoch: 294 Average loss: 0.14916354010303068\n",
      "Epoch: 295 Average loss: 0.14916440990810698\n",
      "Epoch: 296 Average loss: 0.1491652796467777\n",
      "Epoch: 297 Average loss: 0.14916614952684532\n",
      "Epoch: 298 Average loss: 0.14916701927701612\n",
      "Epoch: 299 Average loss: 0.14916788856415958\n",
      "Epoch: 300 Average loss: 0.14916875719359163\n",
      "Epoch: 301 Average loss: 0.14916962543783124\n",
      "Epoch: 302 Average loss: 0.1491704939718719\n",
      "Epoch: 303 Average loss: 0.14917136286319793\n",
      "Epoch: 304 Average loss: 0.14917223161976473\n",
      "Epoch: 305 Average loss: 0.1491731003420897\n",
      "Epoch: 306 Average loss: 0.14917396915145748\n",
      "Epoch: 307 Average loss: 0.14917483753273902\n",
      "Epoch: 308 Average loss: 0.14917570554617116\n",
      "Epoch: 309 Average loss: 0.14917657267671477\n",
      "Epoch: 310 Average loss: 0.14917743944633977\n",
      "Epoch: 311 Average loss: 0.14917830622420472\n",
      "Epoch: 312 Average loss: 0.14917917262587954\n",
      "Epoch: 313 Average loss: 0.1491800390488531\n",
      "Epoch: 314 Average loss: 0.14918090527948333\n",
      "Epoch: 315 Average loss: 0.14918177206630676\n",
      "Epoch: 316 Average loss: 0.1491826384678505\n",
      "Epoch: 317 Average loss: 0.14918350417570048\n",
      "Epoch: 318 Average loss: 0.14918436960094444\n",
      "Epoch: 319 Average loss: 0.14918523517547744\n",
      "Epoch: 320 Average loss: 0.14918610155641232\n",
      "Epoch: 321 Average loss: 0.14918696738686457\n",
      "Epoch: 322 Average loss: 0.1491878330239011\n",
      "Epoch: 323 Average loss: 0.14918869816468874\n",
      "Epoch: 324 Average loss: 0.14918956282007337\n",
      "Epoch: 325 Average loss: 0.1491904270477915\n",
      "Epoch: 326 Average loss: 0.149191291228537\n",
      "Epoch: 327 Average loss: 0.1491921552399939\n",
      "Epoch: 328 Average loss: 0.14919301874688148\n",
      "Epoch: 329 Average loss: 0.14919388184236132\n",
      "Epoch: 330 Average loss: 0.14919474402796679\n",
      "Epoch: 331 Average loss: 0.14919560533732276\n",
      "Epoch: 332 Average loss: 0.14919646658584784\n",
      "Epoch: 333 Average loss: 0.14919732776797268\n",
      "Epoch: 334 Average loss: 0.14919818955949485\n",
      "Epoch: 335 Average loss: 0.14919905084778964\n",
      "Epoch: 336 Average loss: 0.14919991169164643\n",
      "Epoch: 337 Average loss: 0.14920077357056816\n",
      "Epoch: 338 Average loss: 0.14920163611517837\n",
      "Epoch: 339 Average loss: 0.14920249873346247\n",
      "Epoch: 340 Average loss: 0.149203361227499\n",
      "Epoch: 341 Average loss: 0.14920422346567863\n",
      "Epoch: 342 Average loss: 0.14920508551102918\n",
      "Epoch: 343 Average loss: 0.14920594739489973\n",
      "Epoch: 344 Average loss: 0.1492068093617682\n",
      "Epoch: 345 Average loss: 0.149207671453795\n",
      "Epoch: 346 Average loss: 0.14920853341872709\n",
      "Epoch: 347 Average loss: 0.14920939521396193\n",
      "Epoch: 348 Average loss: 0.14921025749017547\n",
      "Epoch: 349 Average loss: 0.14921111913375937\n",
      "Epoch: 350 Average loss: 0.1492119806451461\n",
      "Epoch: 351 Average loss: 0.1492128418386136\n",
      "Epoch: 352 Average loss: 0.1492137027563562\n",
      "Epoch: 353 Average loss: 0.1492145633002994\n",
      "Epoch: 354 Average loss: 0.14921542345042083\n",
      "Epoch: 355 Average loss: 0.14921628359872366\n",
      "Epoch: 356 Average loss: 0.1492171429453857\n",
      "Epoch: 357 Average loss: 0.14921800224738022\n",
      "Epoch: 358 Average loss: 0.14921886200383772\n",
      "Epoch: 359 Average loss: 0.14921972184865645\n",
      "Epoch: 360 Average loss: 0.149220581437392\n",
      "Epoch: 361 Average loss: 0.14922144102203683\n",
      "Epoch: 362 Average loss: 0.14922230014012108\n",
      "Epoch: 363 Average loss: 0.1492231590145786\n",
      "Epoch: 364 Average loss: 0.14922401761996976\n",
      "Epoch: 365 Average loss: 0.14922487602179707\n",
      "Epoch: 366 Average loss: 0.14922573439723796\n",
      "Epoch: 367 Average loss: 0.14922659253843648\n",
      "Epoch: 368 Average loss: 0.14922745065635404\n",
      "Epoch: 369 Average loss: 0.14922830829512307\n",
      "Epoch: 370 Average loss: 0.14922916531159572\n",
      "Epoch: 371 Average loss: 0.1492300216086093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 372 Average loss: 0.14923087754372177\n",
      "Epoch: 373 Average loss: 0.14923173316120453\n",
      "Epoch: 374 Average loss: 0.14923258829046673\n",
      "Epoch: 375 Average loss: 0.14923344267833075\n",
      "Epoch: 376 Average loss: 0.14923429772502095\n",
      "Epoch: 377 Average loss: 0.1492351529320637\n",
      "Epoch: 378 Average loss: 0.1492360088529395\n",
      "Epoch: 379 Average loss: 0.14923686549971157\n",
      "Epoch: 380 Average loss: 0.1492377224988818\n",
      "Epoch: 381 Average loss: 0.14923857965513096\n",
      "Epoch: 382 Average loss: 0.14923943696866274\n",
      "Epoch: 383 Average loss: 0.14924029417080792\n",
      "Epoch: 384 Average loss: 0.14924115086273348\n",
      "Epoch: 385 Average loss: 0.14924200709524038\n",
      "Epoch: 386 Average loss: 0.1492428632615258\n",
      "Epoch: 387 Average loss: 0.1492437192646944\n",
      "Epoch: 388 Average loss: 0.1492445740956953\n",
      "Epoch: 389 Average loss: 0.1492454285697761\n",
      "Epoch: 390 Average loss: 0.14924628257157033\n",
      "Epoch: 391 Average loss: 0.149247135831147\n",
      "Epoch: 392 Average loss: 0.1492479894540498\n",
      "Epoch: 393 Average loss: 0.1492488430266075\n",
      "Epoch: 394 Average loss: 0.1492496963317489\n",
      "Epoch: 395 Average loss: 0.14925054972104396\n",
      "Epoch: 396 Average loss: 0.14925140320596672\n",
      "Epoch: 397 Average loss: 0.14925225668772316\n",
      "Epoch: 398 Average loss: 0.14925311000772742\n",
      "Epoch: 399 Average loss: 0.14925396329380214\n",
      "Epoch: 400 Average loss: 0.149254816507312\n",
      "Epoch: 401 Average loss: 0.1492556695340263\n",
      "Epoch: 402 Average loss: 0.14925652252717217\n",
      "Epoch: 403 Average loss: 0.14925737564873776\n",
      "Epoch: 404 Average loss: 0.14925822828911076\n",
      "Epoch: 405 Average loss: 0.14925908036204213\n",
      "Epoch: 406 Average loss: 0.14925993201213886\n",
      "Epoch: 407 Average loss: 0.14926078365436363\n",
      "Epoch: 408 Average loss: 0.14926163509197188\n",
      "Epoch: 409 Average loss: 0.1492624858042999\n",
      "Epoch: 410 Average loss: 0.1492633362978945\n",
      "Epoch: 411 Average loss: 0.14926418661404472\n",
      "Epoch: 412 Average loss: 0.14926503695009\n",
      "Epoch: 413 Average loss: 0.14926588791144407\n",
      "Epoch: 414 Average loss: 0.14926673946548702\n",
      "Epoch: 415 Average loss: 0.14926759094839695\n",
      "Epoch: 416 Average loss: 0.14926844262913364\n",
      "Epoch: 417 Average loss: 0.1492692941360608\n",
      "Epoch: 418 Average loss: 0.14927014534797797\n",
      "Epoch: 419 Average loss: 0.14927099654224366\n",
      "Epoch: 420 Average loss: 0.14927184759622253\n",
      "Epoch: 421 Average loss: 0.14927269841881677\n",
      "Epoch: 422 Average loss: 0.14927354883511446\n",
      "Epoch: 423 Average loss: 0.14927439879164914\n",
      "Epoch: 424 Average loss: 0.14927524851630164\n",
      "Epoch: 425 Average loss: 0.1492760977418778\n",
      "Epoch: 426 Average loss: 0.149276946497048\n",
      "Epoch: 427 Average loss: 0.14927779432322263\n",
      "Epoch: 428 Average loss: 0.14927864272604557\n",
      "Epoch: 429 Average loss: 0.14927949101652596\n",
      "Epoch: 430 Average loss: 0.1492803391649405\n",
      "Epoch: 431 Average loss: 0.14928118662748702\n",
      "Epoch: 432 Average loss: 0.14928203348866753\n",
      "Epoch: 433 Average loss: 0.1492828797822275\n",
      "Epoch: 434 Average loss: 0.14928372579772034\n",
      "Epoch: 435 Average loss: 0.14928457125961836\n",
      "Epoch: 436 Average loss: 0.14928541706557916\n",
      "Epoch: 437 Average loss: 0.1492862634201549\n",
      "Epoch: 438 Average loss: 0.14928711000028416\n",
      "Epoch: 439 Average loss: 0.14928795659803873\n",
      "Epoch: 440 Average loss: 0.14928880261789387\n",
      "Epoch: 441 Average loss: 0.14928964835039243\n",
      "Epoch: 442 Average loss: 0.14929049417244752\n",
      "Epoch: 443 Average loss: 0.1492913399414693\n",
      "Epoch: 444 Average loss: 0.14929218554645285\n",
      "Epoch: 445 Average loss: 0.14929303158522608\n",
      "Epoch: 446 Average loss: 0.14929387786693882\n",
      "Epoch: 447 Average loss: 0.14929472432010976\n",
      "Epoch: 448 Average loss: 0.1492955705804284\n",
      "Epoch: 449 Average loss: 0.14929641669938665\n",
      "Epoch: 450 Average loss: 0.14929726244879765\n",
      "Epoch: 451 Average loss: 0.14929810758753895\n",
      "Epoch: 452 Average loss: 0.14929895196120582\n",
      "Epoch: 453 Average loss: 0.14929979610909797\n",
      "Epoch: 454 Average loss: 0.14930063959378537\n",
      "Epoch: 455 Average loss: 0.14930148268775947\n",
      "Epoch: 456 Average loss: 0.14930232525004247\n",
      "Epoch: 457 Average loss: 0.14930316786995446\n",
      "Epoch: 458 Average loss: 0.14930401014444544\n",
      "Epoch: 459 Average loss: 0.14930485201518143\n",
      "Epoch: 460 Average loss: 0.14930569375475783\n",
      "Epoch: 461 Average loss: 0.14930653526935747\n",
      "Epoch: 462 Average loss: 0.14930737670724664\n",
      "Epoch: 463 Average loss: 0.1493082186690172\n",
      "Epoch: 464 Average loss: 0.1493090607503846\n",
      "Epoch: 465 Average loss: 0.14930990264989788\n",
      "Epoch: 466 Average loss: 0.14931074548270235\n",
      "Epoch: 467 Average loss: 0.1493115883331849\n",
      "Epoch: 468 Average loss: 0.1493124294279328\n",
      "Epoch: 469 Average loss: 0.14931327023994853\n",
      "Epoch: 470 Average loss: 0.1493141111913721\n",
      "Epoch: 471 Average loss: 0.14931495165904177\n",
      "Epoch: 472 Average loss: 0.1493157913011237\n",
      "Epoch: 473 Average loss: 0.14931663117673666\n",
      "Epoch: 474 Average loss: 0.149317471148368\n",
      "Epoch: 475 Average loss: 0.1493183104822275\n",
      "Epoch: 476 Average loss: 0.1493191496215824\n",
      "Epoch: 477 Average loss: 0.14931998796160964\n",
      "Epoch: 478 Average loss: 0.14932082606928437\n",
      "Epoch: 479 Average loss: 0.1493216640436533\n",
      "Epoch: 480 Average loss: 0.14932250210319306\n",
      "Epoch: 481 Average loss: 0.14932334004166165\n",
      "Epoch: 482 Average loss: 0.1493241779798373\n",
      "Epoch: 483 Average loss: 0.14932501558748376\n",
      "Epoch: 484 Average loss: 0.14932585292187883\n",
      "Epoch: 485 Average loss: 0.1493266903941228\n",
      "Epoch: 486 Average loss: 0.14932752792976753\n",
      "Epoch: 487 Average loss: 0.14932836493657897\n",
      "Epoch: 488 Average loss: 0.1493292020973982\n",
      "Epoch: 489 Average loss: 0.1493300390640027\n",
      "Epoch: 490 Average loss: 0.14933087580105903\n",
      "Epoch: 491 Average loss: 0.14933171208935\n",
      "Epoch: 492 Average loss: 0.14933254829779638\n",
      "Epoch: 493 Average loss: 0.14933338448866676\n",
      "Epoch: 494 Average loss: 0.14933422053886497\n",
      "Epoch: 495 Average loss: 0.14933505601680847\n",
      "Epoch: 496 Average loss: 0.14933588932106018\n",
      "Epoch: 497 Average loss: 0.14933672168465595\n",
      "Epoch: 498 Average loss: 0.1493375539585931\n",
      "Epoch: 499 Average loss: 0.14933838587511344\n",
      "Epoch: 500 Average loss: 0.14933921726149507\n"
     ]
    }
   ],
   "source": [
    "SGD_train_epoch(X_train, y_train, epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:, -4:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiddenLayer.W_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiddenLayer.b_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiddenLayer.forward_propagation(X_train[:, -4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[0]\n",
    "y.size\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
